{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "c063ff71",
   "metadata": {
    "cellId": "0ogihz5v2bdnpgpd7mhfome"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score as rauc\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "235eb2b0",
   "metadata": {
    "cellId": "gfhz6dzp22kgbszh1nqqo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19356</th>\n",
       "      <td>Никогда и ни в коем случае не открывайте счет ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>ТИ откровенно забили на качество и развивают с...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>Я считаю, это прорыв и лидерство финансовых ус...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>Писал мужчина очень доходчиво, не финансовым я...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>Данная ситуация меня сильно выбила из колеи, и...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19361 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment\n",
       "0           При этом всегда получал качественные услуги.         1\n",
       "1      Не вижу, за что хотя бы 2 поставить, сервис на 1!        -1\n",
       "2           Вот так \"Мой любимый\" банк МКБ меня обманул.        -1\n",
       "3                   Отвратительное отношение к клиентам.        -1\n",
       "4      Всегда в любое время дня и ночи помогут, ответ...         1\n",
       "...                                                  ...       ...\n",
       "19356  Никогда и ни в коем случае не открывайте счет ...        -1\n",
       "19357  ТИ откровенно забили на качество и развивают с...        -1\n",
       "19358  Я считаю, это прорыв и лидерство финансовых ус...         1\n",
       "19359  Писал мужчина очень доходчиво, не финансовым я...         1\n",
       "19360  Данная ситуация меня сильно выбила из колеи, и...        -1\n",
       "\n",
       "[19361 rows x 2 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/takimov/MLTinkoff/master/train.csv',encoding='utf-8').drop(columns='Unnamed: 0')\n",
    "df.loc[df['sentiment']=='+', 'sentiment'] = 1\n",
    "df.loc[df['sentiment']=='−', 'sentiment'] = -1\n",
    "df.loc[df['sentiment']=='?', 'sentiment'] = 0\n",
    "df[['sentence','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7bf9dc03",
   "metadata": {
    "cellId": "q8r7bq91jyrripi3obkqz"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "31074db7",
   "metadata": {
    "cellId": "3myq01ve8hfog0f0k5ed4l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56bfd7",
   "metadata": {
    "cellId": "ypbxh2knvne1qyu51u8o9e",
    "execution_id": "cfba83d2-9585-4f56-b1fc-d4355bf8d22e"
   },
   "source": [
    "### Doing for 19361 sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5b1cff94",
   "metadata": {
    "cellId": "t2ym315ftj6icip1wijg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19361/19361 [00:11<00:00, 1734.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "segments_ids_ans = list()\n",
    "indexed_tokens_ans = list()\n",
    "for i in tqdm(range(len(df['sentence']))):\n",
    "    text = df['sentence'][i]\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids_ans.append(segments_ids)\n",
    "    indexed_tokens_ans.append(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9df24ae6",
   "metadata": {
    "cellId": "9dyah1we5pmv5yeiwik1m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19361/19361 [00:00<00:00, 63118.74it/s]\n",
      "100%|██████████| 19361/19361 [00:00<00:00, 95727.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "sentences_count = len(df['sentence'])\n",
    "tokens_tensor_ans = [torch.tensor([indexed_tokens_ans[i]]) for i in tqdm(range(sentences_count))]\n",
    "segments_tensors_ans = [torch.tensor([segments_ids_ans[i]]) for i in tqdm(range(sentences_count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2511a",
   "metadata": {
    "cellId": "c3padzjt5gqjodloa9w0aq",
    "execution_id": "e00ba5e7-b297-41c7-92ca-ed52112cd1ba"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "encoded_layers_ans = list()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(sentences_count)):\n",
    "        encoded_layers, _ = model(tokens_tensor_ans[i], segments_tensors_ans[i])\n",
    "        encoded_layers_ans.append(encoded_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f58aff39",
   "metadata": {
    "cellId": "2s4cnvu7mpedehj9suw6xf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19361/19361 [00:22<00:00, 864.14it/s]\n",
      "100%|██████████| 19361/19361 [00:00<00:00, 236551.95it/s]\n",
      "100%|██████████| 19361/19361 [00:00<00:00, 325679.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "token_embeddings = [torch.stack(encoded_layers_ans[i], dim=0) for i in tqdm(range(sentences_count))]\n",
    "token_embeddings = [torch.squeeze(token_embeddings[i], dim=1) for i in tqdm(range(sentences_count))]\n",
    "token_embeddings = [token_embeddings[i].permute(1,0,2) for i in tqdm(range(sentences_count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "188cc7e1",
   "metadata": {
    "cellId": "qxnvutbt786dmfdtt0q4n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19361/19361 [00:00<00:00, 2074225.28it/s]\n",
      "100%|██████████| 19361/19361 [00:15<00:00, 1245.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "token_vecs_sum = [[] for i in tqdm(range(sentences_count))]\n",
    "for i in tqdm(range(sentences_count)):\n",
    "    for token in token_embeddings[i]:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum[i].append(sum_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9714240c",
   "metadata": {
    "cellId": "28eb8c7x1557cj0v6y8bfy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19361/19361 [00:02<00:00, 9137.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "sentence_embedding_ans = list()\n",
    "for i in tqdm(range(sentences_count)):\n",
    "    token_vecs = encoded_layers_ans[i][11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    sentence_embedding_ans.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d3450bfe",
   "metadata": {
    "cellId": "fa9aa4nbmit1ocgvz3x9a"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "with open ('embedding.npy', 'wb') as f:\n",
    "    np.save(f, np.array(sentence_embedding_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3908fd7f",
   "metadata": {
    "cellId": "kswwl71wjqu58ediorou8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19534642, -0.49691603,  0.66334367, ..., -0.08865543,\n",
       "        -0.04833395, -0.23987795],\n",
       "       [ 0.12297825, -0.41670936,  0.18545565, ...,  0.05767892,\n",
       "        -0.02469435,  0.00142835],\n",
       "       [ 0.1038917 , -0.45776427,  0.41683024, ..., -0.0145911 ,\n",
       "        -0.3258895 , -0.15843569],\n",
       "       ...,\n",
       "       [-0.01610743, -0.27199015,  0.4125188 , ...,  0.07436399,\n",
       "        -0.00652051, -0.23662536],\n",
       "       [ 0.37400064, -0.33378348,  0.5738513 , ...,  0.03479397,\n",
       "        -0.25966772, -0.18580337],\n",
       "       [-0.01634462, -0.36110577,  0.30266383, ...,  0.01920702,\n",
       "        -0.18327518, -0.31647643]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "sentence_embedding_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3a862",
   "metadata": {
    "cellId": "hqgflsxq7xq0l7b2uyj685",
    "execution_id": "7ea7b45a-c25b-49f0-a5fc-d4e6db407d9e"
   },
   "source": [
    "### Model training + tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d6983f3a",
   "metadata": {
    "cellId": "b2njfvedtalnr2xyurl169"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "x_train, x_test, y_train, y_test = train_test_split(sentence_embedding_ans, df['sentiment'], test_size = 0.3, train_size = 0.7, random_state  = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "93773c95",
   "metadata": {
    "cellId": "7ehqxpqe8ifu06d5j4pitg"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "model = CatBoostClassifier(loss_function='MultiClass', eval_metric='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "445b55e7",
   "metadata": {
    "cellId": "iw5nion6i3txzn19lrq2j"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "sentence_embedding_ans = np.array([sentence_embedding_ans[i].numpy() for i in range(sentences_count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3a602fe3",
   "metadata": {
    "cellId": "fvzdzs1cu9tdwxjwi3cx95"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "with open ('sent_embedding.npy', 'wb') as f:\n",
    "    np.save(f, np.array(sentence_embedding_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9a063280",
   "metadata": {
    "cellId": "z04qf73h6gjkuh4kkkvjj",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.090376\n",
      "0:\ttotal: 121ms\tremaining: 2m 1s\n",
      "1:\ttotal: 188ms\tremaining: 1m 33s\n",
      "2:\ttotal: 269ms\tremaining: 1m 29s\n",
      "3:\ttotal: 343ms\tremaining: 1m 25s\n",
      "4:\ttotal: 419ms\tremaining: 1m 23s\n",
      "5:\ttotal: 480ms\tremaining: 1m 19s\n",
      "6:\ttotal: 554ms\tremaining: 1m 18s\n",
      "7:\ttotal: 618ms\tremaining: 1m 16s\n",
      "8:\ttotal: 694ms\tremaining: 1m 16s\n",
      "9:\ttotal: 765ms\tremaining: 1m 15s\n",
      "10:\ttotal: 832ms\tremaining: 1m 14s\n",
      "11:\ttotal: 898ms\tremaining: 1m 13s\n",
      "12:\ttotal: 958ms\tremaining: 1m 12s\n",
      "13:\ttotal: 1.02s\tremaining: 1m 11s\n",
      "14:\ttotal: 1.08s\tremaining: 1m 11s\n",
      "15:\ttotal: 1.15s\tremaining: 1m 10s\n",
      "16:\ttotal: 1.23s\tremaining: 1m 10s\n",
      "17:\ttotal: 1.29s\tremaining: 1m 10s\n",
      "18:\ttotal: 1.35s\tremaining: 1m 9s\n",
      "19:\ttotal: 1.42s\tremaining: 1m 9s\n",
      "20:\ttotal: 1.49s\tremaining: 1m 9s\n",
      "21:\ttotal: 1.55s\tremaining: 1m 9s\n",
      "22:\ttotal: 1.62s\tremaining: 1m 8s\n",
      "23:\ttotal: 1.69s\tremaining: 1m 8s\n",
      "24:\ttotal: 1.76s\tremaining: 1m 8s\n",
      "25:\ttotal: 1.83s\tremaining: 1m 8s\n",
      "26:\ttotal: 1.9s\tremaining: 1m 8s\n",
      "27:\ttotal: 1.97s\tremaining: 1m 8s\n",
      "28:\ttotal: 2.04s\tremaining: 1m 8s\n",
      "29:\ttotal: 2.1s\tremaining: 1m 7s\n",
      "30:\ttotal: 2.16s\tremaining: 1m 7s\n",
      "31:\ttotal: 2.23s\tremaining: 1m 7s\n",
      "32:\ttotal: 2.29s\tremaining: 1m 7s\n",
      "33:\ttotal: 2.36s\tremaining: 1m 7s\n",
      "34:\ttotal: 2.43s\tremaining: 1m 6s\n",
      "35:\ttotal: 2.49s\tremaining: 1m 6s\n",
      "36:\ttotal: 2.56s\tremaining: 1m 6s\n",
      "37:\ttotal: 2.62s\tremaining: 1m 6s\n",
      "38:\ttotal: 2.69s\tremaining: 1m 6s\n",
      "39:\ttotal: 2.75s\tremaining: 1m 6s\n",
      "40:\ttotal: 2.82s\tremaining: 1m 5s\n",
      "41:\ttotal: 2.88s\tremaining: 1m 5s\n",
      "42:\ttotal: 2.95s\tremaining: 1m 5s\n",
      "43:\ttotal: 3.01s\tremaining: 1m 5s\n",
      "44:\ttotal: 3.08s\tremaining: 1m 5s\n",
      "45:\ttotal: 3.15s\tremaining: 1m 5s\n",
      "46:\ttotal: 3.22s\tremaining: 1m 5s\n",
      "47:\ttotal: 3.29s\tremaining: 1m 5s\n",
      "48:\ttotal: 3.36s\tremaining: 1m 5s\n",
      "49:\ttotal: 3.42s\tremaining: 1m 5s\n",
      "50:\ttotal: 3.5s\tremaining: 1m 5s\n",
      "51:\ttotal: 3.57s\tremaining: 1m 5s\n",
      "52:\ttotal: 3.64s\tremaining: 1m 5s\n",
      "53:\ttotal: 3.71s\tremaining: 1m 5s\n",
      "54:\ttotal: 3.79s\tremaining: 1m 5s\n",
      "55:\ttotal: 3.86s\tremaining: 1m 5s\n",
      "56:\ttotal: 3.94s\tremaining: 1m 5s\n",
      "57:\ttotal: 4.01s\tremaining: 1m 5s\n",
      "58:\ttotal: 4.07s\tremaining: 1m 4s\n",
      "59:\ttotal: 4.14s\tremaining: 1m 4s\n",
      "60:\ttotal: 4.21s\tremaining: 1m 4s\n",
      "61:\ttotal: 4.28s\tremaining: 1m 4s\n",
      "62:\ttotal: 4.34s\tremaining: 1m 4s\n",
      "63:\ttotal: 4.42s\tremaining: 1m 4s\n",
      "64:\ttotal: 4.48s\tremaining: 1m 4s\n",
      "65:\ttotal: 4.55s\tremaining: 1m 4s\n",
      "66:\ttotal: 4.61s\tremaining: 1m 4s\n",
      "67:\ttotal: 4.68s\tremaining: 1m 4s\n",
      "68:\ttotal: 4.74s\tremaining: 1m 3s\n",
      "69:\ttotal: 4.8s\tremaining: 1m 3s\n",
      "70:\ttotal: 4.86s\tremaining: 1m 3s\n",
      "71:\ttotal: 4.93s\tremaining: 1m 3s\n",
      "72:\ttotal: 4.99s\tremaining: 1m 3s\n",
      "73:\ttotal: 5.06s\tremaining: 1m 3s\n",
      "74:\ttotal: 5.12s\tremaining: 1m 3s\n",
      "75:\ttotal: 5.19s\tremaining: 1m 3s\n",
      "76:\ttotal: 5.26s\tremaining: 1m 3s\n",
      "77:\ttotal: 5.33s\tremaining: 1m 2s\n",
      "78:\ttotal: 5.41s\tremaining: 1m 3s\n",
      "79:\ttotal: 5.48s\tremaining: 1m 3s\n",
      "80:\ttotal: 5.55s\tremaining: 1m 2s\n",
      "81:\ttotal: 5.62s\tremaining: 1m 2s\n",
      "82:\ttotal: 5.7s\tremaining: 1m 2s\n",
      "83:\ttotal: 5.77s\tremaining: 1m 2s\n",
      "84:\ttotal: 5.84s\tremaining: 1m 2s\n",
      "85:\ttotal: 5.91s\tremaining: 1m 2s\n",
      "86:\ttotal: 5.98s\tremaining: 1m 2s\n",
      "87:\ttotal: 6.05s\tremaining: 1m 2s\n",
      "88:\ttotal: 6.12s\tremaining: 1m 2s\n",
      "89:\ttotal: 6.19s\tremaining: 1m 2s\n",
      "90:\ttotal: 6.25s\tremaining: 1m 2s\n",
      "91:\ttotal: 6.33s\tremaining: 1m 2s\n",
      "92:\ttotal: 6.4s\tremaining: 1m 2s\n",
      "93:\ttotal: 6.48s\tremaining: 1m 2s\n",
      "94:\ttotal: 6.56s\tremaining: 1m 2s\n",
      "95:\ttotal: 6.64s\tremaining: 1m 2s\n",
      "96:\ttotal: 6.71s\tremaining: 1m 2s\n",
      "97:\ttotal: 6.77s\tremaining: 1m 2s\n",
      "98:\ttotal: 6.85s\tremaining: 1m 2s\n",
      "99:\ttotal: 6.93s\tremaining: 1m 2s\n",
      "100:\ttotal: 7s\tremaining: 1m 2s\n",
      "101:\ttotal: 7.08s\tremaining: 1m 2s\n",
      "102:\ttotal: 7.15s\tremaining: 1m 2s\n",
      "103:\ttotal: 7.23s\tremaining: 1m 2s\n",
      "104:\ttotal: 7.31s\tremaining: 1m 2s\n",
      "105:\ttotal: 7.37s\tremaining: 1m 2s\n",
      "106:\ttotal: 7.43s\tremaining: 1m 2s\n",
      "107:\ttotal: 7.5s\tremaining: 1m 1s\n",
      "108:\ttotal: 7.58s\tremaining: 1m 1s\n",
      "109:\ttotal: 7.65s\tremaining: 1m 1s\n",
      "110:\ttotal: 7.72s\tremaining: 1m 1s\n",
      "111:\ttotal: 7.79s\tremaining: 1m 1s\n",
      "112:\ttotal: 7.86s\tremaining: 1m 1s\n",
      "113:\ttotal: 7.93s\tremaining: 1m 1s\n",
      "114:\ttotal: 8s\tremaining: 1m 1s\n",
      "115:\ttotal: 8.08s\tremaining: 1m 1s\n",
      "116:\ttotal: 8.15s\tremaining: 1m 1s\n",
      "117:\ttotal: 8.23s\tremaining: 1m 1s\n",
      "118:\ttotal: 8.3s\tremaining: 1m 1s\n",
      "119:\ttotal: 8.37s\tremaining: 1m 1s\n",
      "120:\ttotal: 8.45s\tremaining: 1m 1s\n",
      "121:\ttotal: 8.51s\tremaining: 1m 1s\n",
      "122:\ttotal: 8.59s\tremaining: 1m 1s\n",
      "123:\ttotal: 8.67s\tremaining: 1m 1s\n",
      "124:\ttotal: 8.74s\tremaining: 1m 1s\n",
      "125:\ttotal: 8.82s\tremaining: 1m 1s\n",
      "126:\ttotal: 8.89s\tremaining: 1m 1s\n",
      "127:\ttotal: 8.96s\tremaining: 1m 1s\n",
      "128:\ttotal: 9.03s\tremaining: 1m\n",
      "129:\ttotal: 9.09s\tremaining: 1m\n",
      "130:\ttotal: 9.16s\tremaining: 1m\n",
      "131:\ttotal: 9.24s\tremaining: 1m\n",
      "132:\ttotal: 9.31s\tremaining: 1m\n",
      "133:\ttotal: 9.38s\tremaining: 1m\n",
      "134:\ttotal: 9.45s\tremaining: 1m\n",
      "135:\ttotal: 9.53s\tremaining: 1m\n",
      "136:\ttotal: 9.6s\tremaining: 1m\n",
      "137:\ttotal: 9.67s\tremaining: 1m\n",
      "138:\ttotal: 9.74s\tremaining: 1m\n",
      "139:\ttotal: 9.81s\tremaining: 1m\n",
      "140:\ttotal: 9.89s\tremaining: 1m\n",
      "141:\ttotal: 9.97s\tremaining: 1m\n",
      "142:\ttotal: 10s\tremaining: 1m\n",
      "143:\ttotal: 10.1s\tremaining: 1m\n",
      "144:\ttotal: 10.2s\tremaining: 1m\n",
      "145:\ttotal: 10.3s\tremaining: 60s\n",
      "146:\ttotal: 10.3s\tremaining: 59.9s\n",
      "147:\ttotal: 10.4s\tremaining: 59.7s\n",
      "148:\ttotal: 10.4s\tremaining: 59.6s\n",
      "149:\ttotal: 10.5s\tremaining: 59.5s\n",
      "150:\ttotal: 10.6s\tremaining: 59.4s\n",
      "151:\ttotal: 10.6s\tremaining: 59.3s\n",
      "152:\ttotal: 10.7s\tremaining: 59.1s\n",
      "153:\ttotal: 10.8s\tremaining: 59.1s\n",
      "154:\ttotal: 10.8s\tremaining: 58.9s\n",
      "155:\ttotal: 10.9s\tremaining: 58.8s\n",
      "156:\ttotal: 10.9s\tremaining: 58.7s\n",
      "157:\ttotal: 11s\tremaining: 58.5s\n",
      "158:\ttotal: 11s\tremaining: 58.4s\n",
      "159:\ttotal: 11.1s\tremaining: 58.4s\n",
      "160:\ttotal: 11.2s\tremaining: 58.4s\n",
      "161:\ttotal: 11.3s\tremaining: 58.3s\n",
      "162:\ttotal: 11.3s\tremaining: 58.2s\n",
      "163:\ttotal: 11.4s\tremaining: 58.1s\n",
      "164:\ttotal: 11.5s\tremaining: 58s\n",
      "165:\ttotal: 11.5s\tremaining: 57.8s\n",
      "166:\ttotal: 11.6s\tremaining: 57.7s\n",
      "167:\ttotal: 11.7s\tremaining: 57.8s\n",
      "168:\ttotal: 11.8s\tremaining: 57.8s\n",
      "169:\ttotal: 11.8s\tremaining: 57.8s\n",
      "170:\ttotal: 11.9s\tremaining: 57.7s\n",
      "171:\ttotal: 12s\tremaining: 57.7s\n",
      "172:\ttotal: 12s\tremaining: 57.6s\n",
      "173:\ttotal: 12.1s\tremaining: 57.5s\n",
      "174:\ttotal: 12.2s\tremaining: 57.5s\n",
      "175:\ttotal: 12.3s\tremaining: 57.5s\n",
      "176:\ttotal: 12.3s\tremaining: 57.4s\n",
      "177:\ttotal: 12.4s\tremaining: 57.3s\n",
      "178:\ttotal: 12.5s\tremaining: 57.3s\n",
      "179:\ttotal: 12.6s\tremaining: 57.3s\n",
      "180:\ttotal: 12.6s\tremaining: 57.1s\n",
      "181:\ttotal: 12.7s\tremaining: 57s\n",
      "182:\ttotal: 12.8s\tremaining: 57s\n",
      "183:\ttotal: 12.8s\tremaining: 57s\n",
      "184:\ttotal: 12.9s\tremaining: 56.9s\n",
      "185:\ttotal: 13s\tremaining: 56.9s\n",
      "186:\ttotal: 13.1s\tremaining: 56.8s\n",
      "187:\ttotal: 13.1s\tremaining: 56.7s\n",
      "188:\ttotal: 13.2s\tremaining: 56.5s\n",
      "189:\ttotal: 13.2s\tremaining: 56.4s\n",
      "190:\ttotal: 13.3s\tremaining: 56.4s\n",
      "191:\ttotal: 13.4s\tremaining: 56.2s\n",
      "192:\ttotal: 13.4s\tremaining: 56.1s\n",
      "193:\ttotal: 13.5s\tremaining: 56s\n",
      "194:\ttotal: 13.5s\tremaining: 55.9s\n",
      "195:\ttotal: 13.6s\tremaining: 55.9s\n",
      "196:\ttotal: 13.7s\tremaining: 55.8s\n",
      "197:\ttotal: 13.8s\tremaining: 55.8s\n",
      "198:\ttotal: 13.8s\tremaining: 55.7s\n",
      "199:\ttotal: 13.9s\tremaining: 55.6s\n",
      "200:\ttotal: 14s\tremaining: 55.5s\n",
      "201:\ttotal: 14s\tremaining: 55.5s\n",
      "202:\ttotal: 14.1s\tremaining: 55.4s\n",
      "203:\ttotal: 14.2s\tremaining: 55.4s\n",
      "204:\ttotal: 14.3s\tremaining: 55.3s\n",
      "205:\ttotal: 14.3s\tremaining: 55.2s\n",
      "206:\ttotal: 14.4s\tremaining: 55.2s\n",
      "207:\ttotal: 14.5s\tremaining: 55.1s\n",
      "208:\ttotal: 14.5s\tremaining: 55s\n",
      "209:\ttotal: 14.6s\tremaining: 55s\n",
      "210:\ttotal: 14.7s\tremaining: 54.9s\n",
      "211:\ttotal: 14.8s\tremaining: 54.9s\n",
      "212:\ttotal: 14.8s\tremaining: 54.9s\n",
      "213:\ttotal: 14.9s\tremaining: 54.9s\n",
      "214:\ttotal: 15.1s\tremaining: 55s\n",
      "215:\ttotal: 15.2s\tremaining: 55s\n",
      "216:\ttotal: 15.3s\tremaining: 55.1s\n",
      "217:\ttotal: 15.4s\tremaining: 55.2s\n",
      "218:\ttotal: 15.5s\tremaining: 55.3s\n",
      "219:\ttotal: 15.6s\tremaining: 55.3s\n",
      "220:\ttotal: 15.7s\tremaining: 55.2s\n",
      "221:\ttotal: 15.7s\tremaining: 55.1s\n",
      "222:\ttotal: 15.8s\tremaining: 55.1s\n",
      "223:\ttotal: 15.9s\tremaining: 55s\n",
      "224:\ttotal: 16s\tremaining: 55s\n",
      "225:\ttotal: 16s\tremaining: 54.9s\n",
      "226:\ttotal: 16.1s\tremaining: 54.8s\n",
      "227:\ttotal: 16.2s\tremaining: 54.8s\n",
      "228:\ttotal: 16.3s\tremaining: 54.8s\n",
      "229:\ttotal: 16.3s\tremaining: 54.7s\n",
      "230:\ttotal: 16.4s\tremaining: 54.6s\n",
      "231:\ttotal: 16.5s\tremaining: 54.6s\n",
      "232:\ttotal: 16.6s\tremaining: 54.5s\n",
      "233:\ttotal: 16.6s\tremaining: 54.5s\n",
      "234:\ttotal: 16.7s\tremaining: 54.4s\n",
      "235:\ttotal: 16.8s\tremaining: 54.4s\n",
      "236:\ttotal: 16.9s\tremaining: 54.4s\n",
      "237:\ttotal: 17s\tremaining: 54.3s\n",
      "238:\ttotal: 17s\tremaining: 54.2s\n",
      "239:\ttotal: 17.1s\tremaining: 54.2s\n",
      "240:\ttotal: 17.2s\tremaining: 54.1s\n",
      "241:\ttotal: 17.3s\tremaining: 54.1s\n",
      "242:\ttotal: 17.3s\tremaining: 54s\n",
      "243:\ttotal: 17.4s\tremaining: 54s\n",
      "244:\ttotal: 17.5s\tremaining: 53.9s\n",
      "245:\ttotal: 17.6s\tremaining: 53.9s\n",
      "246:\ttotal: 17.6s\tremaining: 53.8s\n",
      "247:\ttotal: 17.7s\tremaining: 53.6s\n",
      "248:\ttotal: 17.8s\tremaining: 53.6s\n",
      "249:\ttotal: 17.8s\tremaining: 53.5s\n",
      "250:\ttotal: 17.9s\tremaining: 53.5s\n",
      "251:\ttotal: 18s\tremaining: 53.4s\n",
      "252:\ttotal: 18.1s\tremaining: 53.3s\n",
      "253:\ttotal: 18.1s\tremaining: 53.2s\n",
      "254:\ttotal: 18.2s\tremaining: 53.1s\n",
      "255:\ttotal: 18.2s\tremaining: 53s\n",
      "256:\ttotal: 18.3s\tremaining: 52.9s\n",
      "257:\ttotal: 18.3s\tremaining: 52.7s\n",
      "258:\ttotal: 18.4s\tremaining: 52.6s\n",
      "259:\ttotal: 18.5s\tremaining: 52.5s\n",
      "260:\ttotal: 18.5s\tremaining: 52.4s\n",
      "261:\ttotal: 18.6s\tremaining: 52.3s\n",
      "262:\ttotal: 18.6s\tremaining: 52.2s\n",
      "263:\ttotal: 18.7s\tremaining: 52.1s\n",
      "264:\ttotal: 18.7s\tremaining: 52s\n",
      "265:\ttotal: 18.8s\tremaining: 51.9s\n",
      "266:\ttotal: 18.9s\tremaining: 51.9s\n",
      "267:\ttotal: 19s\tremaining: 51.8s\n",
      "268:\ttotal: 19.1s\tremaining: 51.8s\n",
      "269:\ttotal: 19.1s\tremaining: 51.8s\n",
      "270:\ttotal: 19.2s\tremaining: 51.8s\n",
      "271:\ttotal: 19.3s\tremaining: 51.7s\n",
      "272:\ttotal: 19.4s\tremaining: 51.7s\n",
      "273:\ttotal: 19.5s\tremaining: 51.6s\n",
      "274:\ttotal: 19.5s\tremaining: 51.5s\n",
      "275:\ttotal: 19.6s\tremaining: 51.4s\n",
      "276:\ttotal: 19.7s\tremaining: 51.4s\n",
      "277:\ttotal: 19.8s\tremaining: 51.3s\n",
      "278:\ttotal: 19.8s\tremaining: 51.2s\n",
      "279:\ttotal: 19.9s\tremaining: 51.2s\n",
      "280:\ttotal: 20s\tremaining: 51.1s\n",
      "281:\ttotal: 20s\tremaining: 51s\n",
      "282:\ttotal: 20.1s\tremaining: 51s\n",
      "283:\ttotal: 20.2s\tremaining: 50.9s\n",
      "284:\ttotal: 20.3s\tremaining: 50.9s\n",
      "285:\ttotal: 20.4s\tremaining: 50.8s\n",
      "286:\ttotal: 20.4s\tremaining: 50.8s\n",
      "287:\ttotal: 20.5s\tremaining: 50.8s\n",
      "288:\ttotal: 20.6s\tremaining: 50.7s\n",
      "289:\ttotal: 20.7s\tremaining: 50.7s\n",
      "290:\ttotal: 20.8s\tremaining: 50.6s\n",
      "291:\ttotal: 20.9s\tremaining: 50.6s\n",
      "292:\ttotal: 21s\tremaining: 50.6s\n",
      "293:\ttotal: 21s\tremaining: 50.5s\n",
      "294:\ttotal: 21.1s\tremaining: 50.5s\n",
      "295:\ttotal: 21.2s\tremaining: 50.4s\n",
      "296:\ttotal: 21.3s\tremaining: 50.4s\n",
      "297:\ttotal: 21.4s\tremaining: 50.3s\n",
      "298:\ttotal: 21.4s\tremaining: 50.3s\n",
      "299:\ttotal: 21.5s\tremaining: 50.2s\n",
      "300:\ttotal: 21.6s\tremaining: 50.2s\n",
      "301:\ttotal: 21.7s\tremaining: 50.1s\n",
      "302:\ttotal: 21.7s\tremaining: 50s\n",
      "303:\ttotal: 21.8s\tremaining: 49.9s\n",
      "304:\ttotal: 21.9s\tremaining: 49.9s\n",
      "305:\ttotal: 22s\tremaining: 49.9s\n",
      "306:\ttotal: 22.1s\tremaining: 49.8s\n",
      "307:\ttotal: 22.1s\tremaining: 49.8s\n",
      "308:\ttotal: 22.2s\tremaining: 49.7s\n",
      "309:\ttotal: 22.3s\tremaining: 49.6s\n",
      "310:\ttotal: 22.4s\tremaining: 49.6s\n",
      "311:\ttotal: 22.4s\tremaining: 49.5s\n",
      "312:\ttotal: 22.5s\tremaining: 49.4s\n",
      "313:\ttotal: 22.6s\tremaining: 49.4s\n",
      "314:\ttotal: 22.7s\tremaining: 49.3s\n",
      "315:\ttotal: 22.7s\tremaining: 49.2s\n",
      "316:\ttotal: 22.8s\tremaining: 49.2s\n",
      "317:\ttotal: 22.9s\tremaining: 49.1s\n",
      "318:\ttotal: 23s\tremaining: 49s\n",
      "319:\ttotal: 23s\tremaining: 49s\n",
      "320:\ttotal: 23.1s\tremaining: 48.9s\n",
      "321:\ttotal: 23.2s\tremaining: 48.8s\n",
      "322:\ttotal: 23.2s\tremaining: 48.7s\n",
      "323:\ttotal: 23.3s\tremaining: 48.6s\n",
      "324:\ttotal: 23.4s\tremaining: 48.5s\n",
      "325:\ttotal: 23.4s\tremaining: 48.5s\n",
      "326:\ttotal: 23.5s\tremaining: 48.4s\n",
      "327:\ttotal: 23.6s\tremaining: 48.3s\n",
      "328:\ttotal: 23.6s\tremaining: 48.2s\n",
      "329:\ttotal: 23.7s\tremaining: 48.2s\n",
      "330:\ttotal: 23.8s\tremaining: 48.1s\n",
      "331:\ttotal: 23.9s\tremaining: 48s\n",
      "332:\ttotal: 23.9s\tremaining: 48s\n",
      "333:\ttotal: 24s\tremaining: 47.9s\n",
      "334:\ttotal: 24.1s\tremaining: 47.8s\n",
      "335:\ttotal: 24.2s\tremaining: 47.8s\n",
      "336:\ttotal: 24.3s\tremaining: 47.7s\n",
      "337:\ttotal: 24.3s\tremaining: 47.7s\n",
      "338:\ttotal: 24.4s\tremaining: 47.6s\n",
      "339:\ttotal: 24.5s\tremaining: 47.5s\n",
      "340:\ttotal: 24.6s\tremaining: 47.5s\n",
      "341:\ttotal: 24.6s\tremaining: 47.4s\n",
      "342:\ttotal: 24.7s\tremaining: 47.3s\n",
      "343:\ttotal: 24.8s\tremaining: 47.3s\n",
      "344:\ttotal: 24.9s\tremaining: 47.2s\n",
      "345:\ttotal: 24.9s\tremaining: 47.1s\n",
      "346:\ttotal: 25s\tremaining: 47.1s\n",
      "347:\ttotal: 25.1s\tremaining: 47s\n",
      "348:\ttotal: 25.2s\tremaining: 47s\n",
      "349:\ttotal: 25.2s\tremaining: 46.9s\n",
      "350:\ttotal: 25.3s\tremaining: 46.8s\n",
      "351:\ttotal: 25.4s\tremaining: 46.7s\n",
      "352:\ttotal: 25.5s\tremaining: 46.7s\n",
      "353:\ttotal: 25.5s\tremaining: 46.6s\n",
      "354:\ttotal: 25.6s\tremaining: 46.5s\n",
      "355:\ttotal: 25.7s\tremaining: 46.4s\n",
      "356:\ttotal: 25.7s\tremaining: 46.3s\n",
      "357:\ttotal: 25.8s\tremaining: 46.2s\n",
      "358:\ttotal: 25.8s\tremaining: 46.1s\n",
      "359:\ttotal: 25.9s\tremaining: 46.1s\n",
      "360:\ttotal: 26s\tremaining: 46s\n",
      "361:\ttotal: 26s\tremaining: 45.9s\n",
      "362:\ttotal: 26.1s\tremaining: 45.8s\n",
      "363:\ttotal: 26.2s\tremaining: 45.8s\n",
      "364:\ttotal: 26.3s\tremaining: 45.7s\n",
      "365:\ttotal: 26.4s\tremaining: 45.6s\n",
      "366:\ttotal: 26.4s\tremaining: 45.6s\n",
      "367:\ttotal: 26.5s\tremaining: 45.5s\n",
      "368:\ttotal: 26.6s\tremaining: 45.4s\n",
      "369:\ttotal: 26.6s\tremaining: 45.4s\n",
      "370:\ttotal: 26.7s\tremaining: 45.3s\n",
      "371:\ttotal: 26.8s\tremaining: 45.3s\n",
      "372:\ttotal: 26.9s\tremaining: 45.2s\n",
      "373:\ttotal: 27s\tremaining: 45.1s\n",
      "374:\ttotal: 27s\tremaining: 45s\n",
      "375:\ttotal: 27.1s\tremaining: 45s\n",
      "376:\ttotal: 27.2s\tremaining: 44.9s\n",
      "377:\ttotal: 27.2s\tremaining: 44.8s\n",
      "378:\ttotal: 27.3s\tremaining: 44.7s\n",
      "379:\ttotal: 27.4s\tremaining: 44.7s\n",
      "380:\ttotal: 27.5s\tremaining: 44.6s\n",
      "381:\ttotal: 27.5s\tremaining: 44.5s\n",
      "382:\ttotal: 27.6s\tremaining: 44.5s\n",
      "383:\ttotal: 27.7s\tremaining: 44.4s\n",
      "384:\ttotal: 27.8s\tremaining: 44.4s\n",
      "385:\ttotal: 27.9s\tremaining: 44.3s\n",
      "386:\ttotal: 27.9s\tremaining: 44.3s\n",
      "387:\ttotal: 28s\tremaining: 44.2s\n",
      "388:\ttotal: 28.1s\tremaining: 44.2s\n",
      "389:\ttotal: 28.2s\tremaining: 44.1s\n",
      "390:\ttotal: 28.3s\tremaining: 44s\n",
      "391:\ttotal: 28.4s\tremaining: 44s\n",
      "392:\ttotal: 28.4s\tremaining: 43.9s\n",
      "393:\ttotal: 28.5s\tremaining: 43.8s\n",
      "394:\ttotal: 28.6s\tremaining: 43.8s\n",
      "395:\ttotal: 28.7s\tremaining: 43.7s\n",
      "396:\ttotal: 28.7s\tremaining: 43.7s\n",
      "397:\ttotal: 28.8s\tremaining: 43.6s\n",
      "398:\ttotal: 28.9s\tremaining: 43.5s\n",
      "399:\ttotal: 29s\tremaining: 43.4s\n",
      "400:\ttotal: 29s\tremaining: 43.4s\n",
      "401:\ttotal: 29.1s\tremaining: 43.3s\n",
      "402:\ttotal: 29.2s\tremaining: 43.2s\n",
      "403:\ttotal: 29.3s\tremaining: 43.2s\n",
      "404:\ttotal: 29.3s\tremaining: 43.1s\n",
      "405:\ttotal: 29.4s\tremaining: 43s\n",
      "406:\ttotal: 29.5s\tremaining: 43s\n",
      "407:\ttotal: 29.6s\tremaining: 42.9s\n",
      "408:\ttotal: 29.6s\tremaining: 42.8s\n",
      "409:\ttotal: 29.7s\tremaining: 42.7s\n",
      "410:\ttotal: 29.8s\tremaining: 42.7s\n",
      "411:\ttotal: 29.8s\tremaining: 42.6s\n",
      "412:\ttotal: 29.9s\tremaining: 42.5s\n",
      "413:\ttotal: 30s\tremaining: 42.5s\n",
      "414:\ttotal: 30.1s\tremaining: 42.4s\n",
      "415:\ttotal: 30.1s\tremaining: 42.3s\n",
      "416:\ttotal: 30.2s\tremaining: 42.2s\n",
      "417:\ttotal: 30.3s\tremaining: 42.2s\n",
      "418:\ttotal: 30.3s\tremaining: 42.1s\n",
      "419:\ttotal: 30.4s\tremaining: 42s\n",
      "420:\ttotal: 30.5s\tremaining: 41.9s\n",
      "421:\ttotal: 30.6s\tremaining: 41.9s\n",
      "422:\ttotal: 30.6s\tremaining: 41.8s\n",
      "423:\ttotal: 30.7s\tremaining: 41.7s\n",
      "424:\ttotal: 30.8s\tremaining: 41.6s\n",
      "425:\ttotal: 30.8s\tremaining: 41.6s\n",
      "426:\ttotal: 30.9s\tremaining: 41.5s\n",
      "427:\ttotal: 31s\tremaining: 41.4s\n",
      "428:\ttotal: 31s\tremaining: 41.3s\n",
      "429:\ttotal: 31.1s\tremaining: 41.3s\n",
      "430:\ttotal: 31.2s\tremaining: 41.2s\n",
      "431:\ttotal: 31.3s\tremaining: 41.1s\n",
      "432:\ttotal: 31.3s\tremaining: 41s\n",
      "433:\ttotal: 31.4s\tremaining: 41s\n",
      "434:\ttotal: 31.5s\tremaining: 40.9s\n",
      "435:\ttotal: 31.5s\tremaining: 40.8s\n",
      "436:\ttotal: 31.6s\tremaining: 40.7s\n",
      "437:\ttotal: 31.7s\tremaining: 40.7s\n",
      "438:\ttotal: 31.8s\tremaining: 40.6s\n",
      "439:\ttotal: 31.8s\tremaining: 40.5s\n",
      "440:\ttotal: 31.9s\tremaining: 40.5s\n",
      "441:\ttotal: 32s\tremaining: 40.4s\n",
      "442:\ttotal: 32.1s\tremaining: 40.3s\n",
      "443:\ttotal: 32.2s\tremaining: 40.3s\n",
      "444:\ttotal: 32.2s\tremaining: 40.2s\n",
      "445:\ttotal: 32.3s\tremaining: 40.1s\n",
      "446:\ttotal: 32.4s\tremaining: 40s\n",
      "447:\ttotal: 32.4s\tremaining: 40s\n",
      "448:\ttotal: 32.5s\tremaining: 39.9s\n",
      "449:\ttotal: 32.6s\tremaining: 39.9s\n",
      "450:\ttotal: 32.7s\tremaining: 39.8s\n",
      "451:\ttotal: 32.8s\tremaining: 39.7s\n",
      "452:\ttotal: 32.8s\tremaining: 39.6s\n",
      "453:\ttotal: 32.9s\tremaining: 39.5s\n",
      "454:\ttotal: 32.9s\tremaining: 39.5s\n",
      "455:\ttotal: 33s\tremaining: 39.4s\n",
      "456:\ttotal: 33.1s\tremaining: 39.3s\n",
      "457:\ttotal: 33.2s\tremaining: 39.2s\n",
      "458:\ttotal: 33.2s\tremaining: 39.2s\n",
      "459:\ttotal: 33.3s\tremaining: 39.1s\n",
      "460:\ttotal: 33.4s\tremaining: 39s\n",
      "461:\ttotal: 33.5s\tremaining: 39s\n",
      "462:\ttotal: 33.6s\tremaining: 38.9s\n",
      "463:\ttotal: 33.7s\tremaining: 38.9s\n",
      "464:\ttotal: 33.7s\tremaining: 38.8s\n",
      "465:\ttotal: 33.8s\tremaining: 38.7s\n",
      "466:\ttotal: 33.9s\tremaining: 38.7s\n",
      "467:\ttotal: 34s\tremaining: 38.6s\n",
      "468:\ttotal: 34.1s\tremaining: 38.6s\n",
      "469:\ttotal: 34.1s\tremaining: 38.5s\n",
      "470:\ttotal: 34.2s\tremaining: 38.4s\n",
      "471:\ttotal: 34.3s\tremaining: 38.3s\n",
      "472:\ttotal: 34.3s\tremaining: 38.3s\n",
      "473:\ttotal: 34.4s\tremaining: 38.2s\n",
      "474:\ttotal: 34.5s\tremaining: 38.1s\n",
      "475:\ttotal: 34.6s\tremaining: 38.1s\n",
      "476:\ttotal: 34.6s\tremaining: 38s\n",
      "477:\ttotal: 34.7s\tremaining: 37.9s\n",
      "478:\ttotal: 34.8s\tremaining: 37.8s\n",
      "479:\ttotal: 34.9s\tremaining: 37.8s\n",
      "480:\ttotal: 34.9s\tremaining: 37.7s\n",
      "481:\ttotal: 35s\tremaining: 37.6s\n",
      "482:\ttotal: 35.1s\tremaining: 37.6s\n",
      "483:\ttotal: 35.2s\tremaining: 37.5s\n",
      "484:\ttotal: 35.2s\tremaining: 37.4s\n",
      "485:\ttotal: 35.3s\tremaining: 37.3s\n",
      "486:\ttotal: 35.4s\tremaining: 37.3s\n",
      "487:\ttotal: 35.4s\tremaining: 37.2s\n",
      "488:\ttotal: 35.5s\tremaining: 37.1s\n",
      "489:\ttotal: 35.6s\tremaining: 37s\n",
      "490:\ttotal: 35.7s\tremaining: 37s\n",
      "491:\ttotal: 35.7s\tremaining: 36.9s\n",
      "492:\ttotal: 35.8s\tremaining: 36.8s\n",
      "493:\ttotal: 35.9s\tremaining: 36.7s\n",
      "494:\ttotal: 35.9s\tremaining: 36.7s\n",
      "495:\ttotal: 36s\tremaining: 36.6s\n",
      "496:\ttotal: 36.1s\tremaining: 36.5s\n",
      "497:\ttotal: 36.2s\tremaining: 36.5s\n",
      "498:\ttotal: 36.2s\tremaining: 36.4s\n",
      "499:\ttotal: 36.3s\tremaining: 36.3s\n",
      "500:\ttotal: 36.4s\tremaining: 36.2s\n",
      "501:\ttotal: 36.5s\tremaining: 36.2s\n",
      "502:\ttotal: 36.5s\tremaining: 36.1s\n",
      "503:\ttotal: 36.6s\tremaining: 36s\n",
      "504:\ttotal: 36.6s\tremaining: 35.9s\n",
      "505:\ttotal: 36.7s\tremaining: 35.8s\n",
      "506:\ttotal: 36.8s\tremaining: 35.8s\n",
      "507:\ttotal: 36.8s\tremaining: 35.7s\n",
      "508:\ttotal: 36.9s\tremaining: 35.6s\n",
      "509:\ttotal: 37s\tremaining: 35.6s\n",
      "510:\ttotal: 37.1s\tremaining: 35.5s\n",
      "511:\ttotal: 37.2s\tremaining: 35.4s\n",
      "512:\ttotal: 37.2s\tremaining: 35.3s\n",
      "513:\ttotal: 37.3s\tremaining: 35.3s\n",
      "514:\ttotal: 37.4s\tremaining: 35.2s\n",
      "515:\ttotal: 37.4s\tremaining: 35.1s\n",
      "516:\ttotal: 37.5s\tremaining: 35.1s\n",
      "517:\ttotal: 37.6s\tremaining: 35s\n",
      "518:\ttotal: 37.7s\tremaining: 34.9s\n",
      "519:\ttotal: 37.8s\tremaining: 34.8s\n",
      "520:\ttotal: 37.8s\tremaining: 34.8s\n",
      "521:\ttotal: 37.9s\tremaining: 34.7s\n",
      "522:\ttotal: 38s\tremaining: 34.7s\n",
      "523:\ttotal: 38.1s\tremaining: 34.6s\n",
      "524:\ttotal: 38.1s\tremaining: 34.5s\n",
      "525:\ttotal: 38.2s\tremaining: 34.5s\n",
      "526:\ttotal: 38.3s\tremaining: 34.4s\n",
      "527:\ttotal: 38.4s\tremaining: 34.3s\n",
      "528:\ttotal: 38.5s\tremaining: 34.2s\n",
      "529:\ttotal: 38.5s\tremaining: 34.2s\n",
      "530:\ttotal: 38.6s\tremaining: 34.1s\n",
      "531:\ttotal: 38.6s\tremaining: 34s\n",
      "532:\ttotal: 38.7s\tremaining: 33.9s\n",
      "533:\ttotal: 38.7s\tremaining: 33.8s\n",
      "534:\ttotal: 38.8s\tremaining: 33.7s\n",
      "535:\ttotal: 38.9s\tremaining: 33.6s\n",
      "536:\ttotal: 38.9s\tremaining: 33.5s\n",
      "537:\ttotal: 39s\tremaining: 33.5s\n",
      "538:\ttotal: 39s\tremaining: 33.4s\n",
      "539:\ttotal: 39.1s\tremaining: 33.3s\n",
      "540:\ttotal: 39.2s\tremaining: 33.2s\n",
      "541:\ttotal: 39.2s\tremaining: 33.1s\n",
      "542:\ttotal: 39.3s\tremaining: 33.1s\n",
      "543:\ttotal: 39.4s\tremaining: 33s\n",
      "544:\ttotal: 39.4s\tremaining: 32.9s\n",
      "545:\ttotal: 39.5s\tremaining: 32.8s\n",
      "546:\ttotal: 39.6s\tremaining: 32.8s\n",
      "547:\ttotal: 39.6s\tremaining: 32.7s\n",
      "548:\ttotal: 39.7s\tremaining: 32.6s\n",
      "549:\ttotal: 39.8s\tremaining: 32.5s\n",
      "550:\ttotal: 39.9s\tremaining: 32.5s\n",
      "551:\ttotal: 39.9s\tremaining: 32.4s\n",
      "552:\ttotal: 40s\tremaining: 32.3s\n",
      "553:\ttotal: 40.1s\tremaining: 32.3s\n",
      "554:\ttotal: 40.2s\tremaining: 32.2s\n",
      "555:\ttotal: 40.3s\tremaining: 32.2s\n",
      "556:\ttotal: 40.4s\tremaining: 32.1s\n",
      "557:\ttotal: 40.5s\tremaining: 32s\n",
      "558:\ttotal: 40.5s\tremaining: 32s\n",
      "559:\ttotal: 40.6s\tremaining: 31.9s\n",
      "560:\ttotal: 40.7s\tremaining: 31.8s\n",
      "561:\ttotal: 40.8s\tremaining: 31.8s\n",
      "562:\ttotal: 40.8s\tremaining: 31.7s\n",
      "563:\ttotal: 40.9s\tremaining: 31.6s\n",
      "564:\ttotal: 41s\tremaining: 31.6s\n",
      "565:\ttotal: 41.1s\tremaining: 31.5s\n",
      "566:\ttotal: 41.1s\tremaining: 31.4s\n",
      "567:\ttotal: 41.2s\tremaining: 31.3s\n",
      "568:\ttotal: 41.3s\tremaining: 31.3s\n",
      "569:\ttotal: 41.4s\tremaining: 31.2s\n",
      "570:\ttotal: 41.4s\tremaining: 31.1s\n",
      "571:\ttotal: 41.5s\tremaining: 31.1s\n",
      "572:\ttotal: 41.6s\tremaining: 31s\n",
      "573:\ttotal: 41.7s\tremaining: 30.9s\n",
      "574:\ttotal: 41.7s\tremaining: 30.9s\n",
      "575:\ttotal: 41.8s\tremaining: 30.8s\n",
      "576:\ttotal: 41.9s\tremaining: 30.7s\n",
      "577:\ttotal: 42s\tremaining: 30.6s\n",
      "578:\ttotal: 42s\tremaining: 30.5s\n",
      "579:\ttotal: 42.1s\tremaining: 30.5s\n",
      "580:\ttotal: 42.1s\tremaining: 30.4s\n",
      "581:\ttotal: 42.2s\tremaining: 30.3s\n",
      "582:\ttotal: 42.3s\tremaining: 30.2s\n",
      "583:\ttotal: 42.4s\tremaining: 30.2s\n",
      "584:\ttotal: 42.4s\tremaining: 30.1s\n",
      "585:\ttotal: 42.5s\tremaining: 30s\n",
      "586:\ttotal: 42.6s\tremaining: 29.9s\n",
      "587:\ttotal: 42.7s\tremaining: 29.9s\n",
      "588:\ttotal: 42.7s\tremaining: 29.8s\n",
      "589:\ttotal: 42.8s\tremaining: 29.7s\n",
      "590:\ttotal: 42.9s\tremaining: 29.7s\n",
      "591:\ttotal: 43s\tremaining: 29.6s\n",
      "592:\ttotal: 43s\tremaining: 29.5s\n",
      "593:\ttotal: 43.1s\tremaining: 29.5s\n",
      "594:\ttotal: 43.2s\tremaining: 29.4s\n",
      "595:\ttotal: 43.2s\tremaining: 29.3s\n",
      "596:\ttotal: 43.3s\tremaining: 29.2s\n",
      "597:\ttotal: 43.4s\tremaining: 29.2s\n",
      "598:\ttotal: 43.5s\tremaining: 29.1s\n",
      "599:\ttotal: 43.5s\tremaining: 29s\n",
      "600:\ttotal: 43.6s\tremaining: 29s\n",
      "601:\ttotal: 43.7s\tremaining: 28.9s\n",
      "602:\ttotal: 43.8s\tremaining: 28.8s\n",
      "603:\ttotal: 43.9s\tremaining: 28.8s\n",
      "604:\ttotal: 44s\tremaining: 28.7s\n",
      "605:\ttotal: 44s\tremaining: 28.6s\n",
      "606:\ttotal: 44.1s\tremaining: 28.5s\n",
      "607:\ttotal: 44.1s\tremaining: 28.5s\n",
      "608:\ttotal: 44.2s\tremaining: 28.4s\n",
      "609:\ttotal: 44.3s\tremaining: 28.3s\n",
      "610:\ttotal: 44.3s\tremaining: 28.2s\n",
      "611:\ttotal: 44.4s\tremaining: 28.2s\n",
      "612:\ttotal: 44.5s\tremaining: 28.1s\n",
      "613:\ttotal: 44.6s\tremaining: 28s\n",
      "614:\ttotal: 44.6s\tremaining: 27.9s\n",
      "615:\ttotal: 44.7s\tremaining: 27.9s\n",
      "616:\ttotal: 44.8s\tremaining: 27.8s\n",
      "617:\ttotal: 44.8s\tremaining: 27.7s\n",
      "618:\ttotal: 44.9s\tremaining: 27.6s\n",
      "619:\ttotal: 45s\tremaining: 27.6s\n",
      "620:\ttotal: 45s\tremaining: 27.5s\n",
      "621:\ttotal: 45.1s\tremaining: 27.4s\n",
      "622:\ttotal: 45.1s\tremaining: 27.3s\n",
      "623:\ttotal: 45.2s\tremaining: 27.2s\n",
      "624:\ttotal: 45.2s\tremaining: 27.1s\n",
      "625:\ttotal: 45.3s\tremaining: 27.1s\n",
      "626:\ttotal: 45.4s\tremaining: 27s\n",
      "627:\ttotal: 45.4s\tremaining: 26.9s\n",
      "628:\ttotal: 45.5s\tremaining: 26.8s\n",
      "629:\ttotal: 45.5s\tremaining: 26.7s\n",
      "630:\ttotal: 45.6s\tremaining: 26.7s\n",
      "631:\ttotal: 45.7s\tremaining: 26.6s\n",
      "632:\ttotal: 45.8s\tremaining: 26.5s\n",
      "633:\ttotal: 45.8s\tremaining: 26.5s\n",
      "634:\ttotal: 45.9s\tremaining: 26.4s\n",
      "635:\ttotal: 46s\tremaining: 26.3s\n",
      "636:\ttotal: 46.1s\tremaining: 26.2s\n",
      "637:\ttotal: 46.1s\tremaining: 26.2s\n",
      "638:\ttotal: 46.2s\tremaining: 26.1s\n",
      "639:\ttotal: 46.3s\tremaining: 26s\n",
      "640:\ttotal: 46.4s\tremaining: 26s\n",
      "641:\ttotal: 46.5s\tremaining: 25.9s\n",
      "642:\ttotal: 46.5s\tremaining: 25.8s\n",
      "643:\ttotal: 46.6s\tremaining: 25.8s\n",
      "644:\ttotal: 46.7s\tremaining: 25.7s\n",
      "645:\ttotal: 46.8s\tremaining: 25.6s\n",
      "646:\ttotal: 46.9s\tremaining: 25.6s\n",
      "647:\ttotal: 46.9s\tremaining: 25.5s\n",
      "648:\ttotal: 47s\tremaining: 25.4s\n",
      "649:\ttotal: 47.1s\tremaining: 25.4s\n",
      "650:\ttotal: 47.2s\tremaining: 25.3s\n",
      "651:\ttotal: 47.2s\tremaining: 25.2s\n",
      "652:\ttotal: 47.3s\tremaining: 25.1s\n",
      "653:\ttotal: 47.4s\tremaining: 25.1s\n",
      "654:\ttotal: 47.5s\tremaining: 25s\n",
      "655:\ttotal: 47.5s\tremaining: 24.9s\n",
      "656:\ttotal: 47.6s\tremaining: 24.9s\n",
      "657:\ttotal: 47.7s\tremaining: 24.8s\n",
      "658:\ttotal: 47.8s\tremaining: 24.7s\n",
      "659:\ttotal: 47.8s\tremaining: 24.6s\n",
      "660:\ttotal: 47.9s\tremaining: 24.6s\n",
      "661:\ttotal: 48s\tremaining: 24.5s\n",
      "662:\ttotal: 48s\tremaining: 24.4s\n",
      "663:\ttotal: 48.1s\tremaining: 24.3s\n",
      "664:\ttotal: 48.2s\tremaining: 24.3s\n",
      "665:\ttotal: 48.2s\tremaining: 24.2s\n",
      "666:\ttotal: 48.3s\tremaining: 24.1s\n",
      "667:\ttotal: 48.3s\tremaining: 24s\n",
      "668:\ttotal: 48.4s\tremaining: 24s\n",
      "669:\ttotal: 48.5s\tremaining: 23.9s\n",
      "670:\ttotal: 48.6s\tremaining: 23.8s\n",
      "671:\ttotal: 48.6s\tremaining: 23.7s\n",
      "672:\ttotal: 48.7s\tremaining: 23.7s\n",
      "673:\ttotal: 48.8s\tremaining: 23.6s\n",
      "674:\ttotal: 48.8s\tremaining: 23.5s\n",
      "675:\ttotal: 48.9s\tremaining: 23.4s\n",
      "676:\ttotal: 49s\tremaining: 23.4s\n",
      "677:\ttotal: 49.1s\tremaining: 23.3s\n",
      "678:\ttotal: 49.1s\tremaining: 23.2s\n",
      "679:\ttotal: 49.2s\tremaining: 23.2s\n",
      "680:\ttotal: 49.3s\tremaining: 23.1s\n",
      "681:\ttotal: 49.4s\tremaining: 23s\n",
      "682:\ttotal: 49.4s\tremaining: 22.9s\n",
      "683:\ttotal: 49.5s\tremaining: 22.9s\n",
      "684:\ttotal: 49.6s\tremaining: 22.8s\n",
      "685:\ttotal: 49.7s\tremaining: 22.7s\n",
      "686:\ttotal: 49.7s\tremaining: 22.7s\n",
      "687:\ttotal: 49.8s\tremaining: 22.6s\n",
      "688:\ttotal: 49.9s\tremaining: 22.5s\n",
      "689:\ttotal: 49.9s\tremaining: 22.4s\n",
      "690:\ttotal: 50s\tremaining: 22.4s\n",
      "691:\ttotal: 50.1s\tremaining: 22.3s\n",
      "692:\ttotal: 50.1s\tremaining: 22.2s\n",
      "693:\ttotal: 50.2s\tremaining: 22.1s\n",
      "694:\ttotal: 50.3s\tremaining: 22.1s\n",
      "695:\ttotal: 50.4s\tremaining: 22s\n",
      "696:\ttotal: 50.5s\tremaining: 21.9s\n",
      "697:\ttotal: 50.5s\tremaining: 21.9s\n",
      "698:\ttotal: 50.6s\tremaining: 21.8s\n",
      "699:\ttotal: 50.6s\tremaining: 21.7s\n",
      "700:\ttotal: 50.7s\tremaining: 21.6s\n",
      "701:\ttotal: 50.8s\tremaining: 21.5s\n",
      "702:\ttotal: 50.8s\tremaining: 21.5s\n",
      "703:\ttotal: 50.9s\tremaining: 21.4s\n",
      "704:\ttotal: 50.9s\tremaining: 21.3s\n",
      "705:\ttotal: 51s\tremaining: 21.2s\n",
      "706:\ttotal: 51.1s\tremaining: 21.2s\n",
      "707:\ttotal: 51.2s\tremaining: 21.1s\n",
      "708:\ttotal: 51.2s\tremaining: 21s\n",
      "709:\ttotal: 51.3s\tremaining: 20.9s\n",
      "710:\ttotal: 51.3s\tremaining: 20.9s\n",
      "711:\ttotal: 51.4s\tremaining: 20.8s\n",
      "712:\ttotal: 51.4s\tremaining: 20.7s\n",
      "713:\ttotal: 51.5s\tremaining: 20.6s\n",
      "714:\ttotal: 51.5s\tremaining: 20.5s\n",
      "715:\ttotal: 51.6s\tremaining: 20.5s\n",
      "716:\ttotal: 51.7s\tremaining: 20.4s\n",
      "717:\ttotal: 51.8s\tremaining: 20.3s\n",
      "718:\ttotal: 51.8s\tremaining: 20.3s\n",
      "719:\ttotal: 51.9s\tremaining: 20.2s\n",
      "720:\ttotal: 52s\tremaining: 20.1s\n",
      "721:\ttotal: 52.1s\tremaining: 20s\n",
      "722:\ttotal: 52.1s\tremaining: 20s\n",
      "723:\ttotal: 52.2s\tremaining: 19.9s\n",
      "724:\ttotal: 52.2s\tremaining: 19.8s\n",
      "725:\ttotal: 52.3s\tremaining: 19.7s\n",
      "726:\ttotal: 52.4s\tremaining: 19.7s\n",
      "727:\ttotal: 52.4s\tremaining: 19.6s\n",
      "728:\ttotal: 52.5s\tremaining: 19.5s\n",
      "729:\ttotal: 52.6s\tremaining: 19.4s\n",
      "730:\ttotal: 52.6s\tremaining: 19.4s\n",
      "731:\ttotal: 52.7s\tremaining: 19.3s\n",
      "732:\ttotal: 52.7s\tremaining: 19.2s\n",
      "733:\ttotal: 52.8s\tremaining: 19.1s\n",
      "734:\ttotal: 52.9s\tremaining: 19.1s\n",
      "735:\ttotal: 52.9s\tremaining: 19s\n",
      "736:\ttotal: 53s\tremaining: 18.9s\n",
      "737:\ttotal: 53s\tremaining: 18.8s\n",
      "738:\ttotal: 53.1s\tremaining: 18.8s\n",
      "739:\ttotal: 53.1s\tremaining: 18.7s\n",
      "740:\ttotal: 53.2s\tremaining: 18.6s\n",
      "741:\ttotal: 53.3s\tremaining: 18.5s\n",
      "742:\ttotal: 53.3s\tremaining: 18.4s\n",
      "743:\ttotal: 53.4s\tremaining: 18.4s\n",
      "744:\ttotal: 53.4s\tremaining: 18.3s\n",
      "745:\ttotal: 53.5s\tremaining: 18.2s\n",
      "746:\ttotal: 53.6s\tremaining: 18.1s\n",
      "747:\ttotal: 53.6s\tremaining: 18.1s\n",
      "748:\ttotal: 53.7s\tremaining: 18s\n",
      "749:\ttotal: 53.8s\tremaining: 17.9s\n",
      "750:\ttotal: 53.9s\tremaining: 17.9s\n",
      "751:\ttotal: 53.9s\tremaining: 17.8s\n",
      "752:\ttotal: 54s\tremaining: 17.7s\n",
      "753:\ttotal: 54.1s\tremaining: 17.7s\n",
      "754:\ttotal: 54.2s\tremaining: 17.6s\n",
      "755:\ttotal: 54.2s\tremaining: 17.5s\n",
      "756:\ttotal: 54.3s\tremaining: 17.4s\n",
      "757:\ttotal: 54.4s\tremaining: 17.4s\n",
      "758:\ttotal: 54.5s\tremaining: 17.3s\n",
      "759:\ttotal: 54.5s\tremaining: 17.2s\n",
      "760:\ttotal: 54.6s\tremaining: 17.2s\n",
      "761:\ttotal: 54.7s\tremaining: 17.1s\n",
      "762:\ttotal: 54.8s\tremaining: 17s\n",
      "763:\ttotal: 54.9s\tremaining: 16.9s\n",
      "764:\ttotal: 54.9s\tremaining: 16.9s\n",
      "765:\ttotal: 55s\tremaining: 16.8s\n",
      "766:\ttotal: 55.1s\tremaining: 16.7s\n",
      "767:\ttotal: 55.2s\tremaining: 16.7s\n",
      "768:\ttotal: 55.2s\tremaining: 16.6s\n",
      "769:\ttotal: 55.3s\tremaining: 16.5s\n",
      "770:\ttotal: 55.4s\tremaining: 16.4s\n",
      "771:\ttotal: 55.5s\tremaining: 16.4s\n",
      "772:\ttotal: 55.5s\tremaining: 16.3s\n",
      "773:\ttotal: 55.6s\tremaining: 16.2s\n",
      "774:\ttotal: 55.7s\tremaining: 16.2s\n",
      "775:\ttotal: 55.8s\tremaining: 16.1s\n",
      "776:\ttotal: 55.9s\tremaining: 16s\n",
      "777:\ttotal: 56s\tremaining: 16s\n",
      "778:\ttotal: 56.1s\tremaining: 15.9s\n",
      "779:\ttotal: 56.1s\tremaining: 15.8s\n",
      "780:\ttotal: 56.2s\tremaining: 15.8s\n",
      "781:\ttotal: 56.3s\tremaining: 15.7s\n",
      "782:\ttotal: 56.4s\tremaining: 15.6s\n",
      "783:\ttotal: 56.5s\tremaining: 15.6s\n",
      "784:\ttotal: 56.5s\tremaining: 15.5s\n",
      "785:\ttotal: 56.6s\tremaining: 15.4s\n",
      "786:\ttotal: 56.7s\tremaining: 15.3s\n",
      "787:\ttotal: 56.8s\tremaining: 15.3s\n",
      "788:\ttotal: 56.8s\tremaining: 15.2s\n",
      "789:\ttotal: 56.9s\tremaining: 15.1s\n",
      "790:\ttotal: 57s\tremaining: 15s\n",
      "791:\ttotal: 57s\tremaining: 15s\n",
      "792:\ttotal: 57.1s\tremaining: 14.9s\n",
      "793:\ttotal: 57.1s\tremaining: 14.8s\n",
      "794:\ttotal: 57.2s\tremaining: 14.8s\n",
      "795:\ttotal: 57.3s\tremaining: 14.7s\n",
      "796:\ttotal: 57.3s\tremaining: 14.6s\n",
      "797:\ttotal: 57.4s\tremaining: 14.5s\n",
      "798:\ttotal: 57.5s\tremaining: 14.5s\n",
      "799:\ttotal: 57.6s\tremaining: 14.4s\n",
      "800:\ttotal: 57.6s\tremaining: 14.3s\n",
      "801:\ttotal: 57.7s\tremaining: 14.2s\n",
      "802:\ttotal: 57.7s\tremaining: 14.2s\n",
      "803:\ttotal: 57.8s\tremaining: 14.1s\n",
      "804:\ttotal: 57.9s\tremaining: 14s\n",
      "805:\ttotal: 57.9s\tremaining: 13.9s\n",
      "806:\ttotal: 58s\tremaining: 13.9s\n",
      "807:\ttotal: 58s\tremaining: 13.8s\n",
      "808:\ttotal: 58.1s\tremaining: 13.7s\n",
      "809:\ttotal: 58.2s\tremaining: 13.6s\n",
      "810:\ttotal: 58.2s\tremaining: 13.6s\n",
      "811:\ttotal: 58.3s\tremaining: 13.5s\n",
      "812:\ttotal: 58.4s\tremaining: 13.4s\n",
      "813:\ttotal: 58.4s\tremaining: 13.4s\n",
      "814:\ttotal: 58.5s\tremaining: 13.3s\n",
      "815:\ttotal: 58.5s\tremaining: 13.2s\n",
      "816:\ttotal: 58.6s\tremaining: 13.1s\n",
      "817:\ttotal: 58.7s\tremaining: 13.1s\n",
      "818:\ttotal: 58.8s\tremaining: 13s\n",
      "819:\ttotal: 58.8s\tremaining: 12.9s\n",
      "820:\ttotal: 58.9s\tremaining: 12.8s\n",
      "821:\ttotal: 59s\tremaining: 12.8s\n",
      "822:\ttotal: 59.1s\tremaining: 12.7s\n",
      "823:\ttotal: 59.2s\tremaining: 12.6s\n",
      "824:\ttotal: 59.2s\tremaining: 12.6s\n",
      "825:\ttotal: 59.3s\tremaining: 12.5s\n",
      "826:\ttotal: 59.4s\tremaining: 12.4s\n",
      "827:\ttotal: 59.4s\tremaining: 12.3s\n",
      "828:\ttotal: 59.5s\tremaining: 12.3s\n",
      "829:\ttotal: 59.6s\tremaining: 12.2s\n",
      "830:\ttotal: 59.6s\tremaining: 12.1s\n",
      "831:\ttotal: 59.7s\tremaining: 12s\n",
      "832:\ttotal: 59.7s\tremaining: 12s\n",
      "833:\ttotal: 59.8s\tremaining: 11.9s\n",
      "834:\ttotal: 59.8s\tremaining: 11.8s\n",
      "835:\ttotal: 59.9s\tremaining: 11.8s\n",
      "836:\ttotal: 60s\tremaining: 11.7s\n",
      "837:\ttotal: 1m\tremaining: 11.6s\n",
      "838:\ttotal: 1m\tremaining: 11.5s\n",
      "839:\ttotal: 1m\tremaining: 11.5s\n",
      "840:\ttotal: 1m\tremaining: 11.4s\n",
      "841:\ttotal: 1m\tremaining: 11.3s\n",
      "842:\ttotal: 1m\tremaining: 11.2s\n",
      "843:\ttotal: 1m\tremaining: 11.2s\n",
      "844:\ttotal: 1m\tremaining: 11.1s\n",
      "845:\ttotal: 1m\tremaining: 11s\n",
      "846:\ttotal: 1m\tremaining: 11s\n",
      "847:\ttotal: 1m\tremaining: 10.9s\n",
      "848:\ttotal: 1m\tremaining: 10.8s\n",
      "849:\ttotal: 1m\tremaining: 10.8s\n",
      "850:\ttotal: 1m 1s\tremaining: 10.7s\n",
      "851:\ttotal: 1m 1s\tremaining: 10.6s\n",
      "852:\ttotal: 1m 1s\tremaining: 10.5s\n",
      "853:\ttotal: 1m 1s\tremaining: 10.5s\n",
      "854:\ttotal: 1m 1s\tremaining: 10.4s\n",
      "855:\ttotal: 1m 1s\tremaining: 10.3s\n",
      "856:\ttotal: 1m 1s\tremaining: 10.3s\n",
      "857:\ttotal: 1m 1s\tremaining: 10.2s\n",
      "858:\ttotal: 1m 1s\tremaining: 10.1s\n",
      "859:\ttotal: 1m 1s\tremaining: 10s\n",
      "860:\ttotal: 1m 1s\tremaining: 9.96s\n",
      "861:\ttotal: 1m 1s\tremaining: 9.89s\n",
      "862:\ttotal: 1m 1s\tremaining: 9.82s\n",
      "863:\ttotal: 1m 1s\tremaining: 9.74s\n",
      "864:\ttotal: 1m 1s\tremaining: 9.68s\n",
      "865:\ttotal: 1m 2s\tremaining: 9.6s\n",
      "866:\ttotal: 1m 2s\tremaining: 9.53s\n",
      "867:\ttotal: 1m 2s\tremaining: 9.46s\n",
      "868:\ttotal: 1m 2s\tremaining: 9.38s\n",
      "869:\ttotal: 1m 2s\tremaining: 9.31s\n",
      "870:\ttotal: 1m 2s\tremaining: 9.24s\n",
      "871:\ttotal: 1m 2s\tremaining: 9.17s\n",
      "872:\ttotal: 1m 2s\tremaining: 9.09s\n",
      "873:\ttotal: 1m 2s\tremaining: 9.02s\n",
      "874:\ttotal: 1m 2s\tremaining: 8.95s\n",
      "875:\ttotal: 1m 2s\tremaining: 8.88s\n",
      "876:\ttotal: 1m 2s\tremaining: 8.8s\n",
      "877:\ttotal: 1m 2s\tremaining: 8.73s\n",
      "878:\ttotal: 1m 2s\tremaining: 8.66s\n",
      "879:\ttotal: 1m 2s\tremaining: 8.59s\n",
      "880:\ttotal: 1m 3s\tremaining: 8.52s\n",
      "881:\ttotal: 1m 3s\tremaining: 8.45s\n",
      "882:\ttotal: 1m 3s\tremaining: 8.37s\n",
      "883:\ttotal: 1m 3s\tremaining: 8.3s\n",
      "884:\ttotal: 1m 3s\tremaining: 8.23s\n",
      "885:\ttotal: 1m 3s\tremaining: 8.16s\n",
      "886:\ttotal: 1m 3s\tremaining: 8.09s\n",
      "887:\ttotal: 1m 3s\tremaining: 8.02s\n",
      "888:\ttotal: 1m 3s\tremaining: 7.95s\n",
      "889:\ttotal: 1m 3s\tremaining: 7.88s\n",
      "890:\ttotal: 1m 3s\tremaining: 7.81s\n",
      "891:\ttotal: 1m 3s\tremaining: 7.74s\n",
      "892:\ttotal: 1m 3s\tremaining: 7.67s\n",
      "893:\ttotal: 1m 4s\tremaining: 7.59s\n",
      "894:\ttotal: 1m 4s\tremaining: 7.52s\n",
      "895:\ttotal: 1m 4s\tremaining: 7.45s\n",
      "896:\ttotal: 1m 4s\tremaining: 7.38s\n",
      "897:\ttotal: 1m 4s\tremaining: 7.31s\n",
      "898:\ttotal: 1m 4s\tremaining: 7.24s\n",
      "899:\ttotal: 1m 4s\tremaining: 7.17s\n",
      "900:\ttotal: 1m 4s\tremaining: 7.09s\n",
      "901:\ttotal: 1m 4s\tremaining: 7.02s\n",
      "902:\ttotal: 1m 4s\tremaining: 6.95s\n",
      "903:\ttotal: 1m 4s\tremaining: 6.88s\n",
      "904:\ttotal: 1m 4s\tremaining: 6.81s\n",
      "905:\ttotal: 1m 4s\tremaining: 6.74s\n",
      "906:\ttotal: 1m 5s\tremaining: 6.67s\n",
      "907:\ttotal: 1m 5s\tremaining: 6.59s\n",
      "908:\ttotal: 1m 5s\tremaining: 6.52s\n",
      "909:\ttotal: 1m 5s\tremaining: 6.45s\n",
      "910:\ttotal: 1m 5s\tremaining: 6.37s\n",
      "911:\ttotal: 1m 5s\tremaining: 6.3s\n",
      "912:\ttotal: 1m 5s\tremaining: 6.23s\n",
      "913:\ttotal: 1m 5s\tremaining: 6.16s\n",
      "914:\ttotal: 1m 5s\tremaining: 6.09s\n",
      "915:\ttotal: 1m 5s\tremaining: 6.02s\n",
      "916:\ttotal: 1m 5s\tremaining: 5.95s\n",
      "917:\ttotal: 1m 5s\tremaining: 5.88s\n",
      "918:\ttotal: 1m 5s\tremaining: 5.8s\n",
      "919:\ttotal: 1m 5s\tremaining: 5.73s\n",
      "920:\ttotal: 1m 5s\tremaining: 5.66s\n",
      "921:\ttotal: 1m 6s\tremaining: 5.59s\n",
      "922:\ttotal: 1m 6s\tremaining: 5.51s\n",
      "923:\ttotal: 1m 6s\tremaining: 5.44s\n",
      "924:\ttotal: 1m 6s\tremaining: 5.37s\n",
      "925:\ttotal: 1m 6s\tremaining: 5.3s\n",
      "926:\ttotal: 1m 6s\tremaining: 5.23s\n",
      "927:\ttotal: 1m 6s\tremaining: 5.16s\n",
      "928:\ttotal: 1m 6s\tremaining: 5.08s\n",
      "929:\ttotal: 1m 6s\tremaining: 5.01s\n",
      "930:\ttotal: 1m 6s\tremaining: 4.94s\n",
      "931:\ttotal: 1m 6s\tremaining: 4.87s\n",
      "932:\ttotal: 1m 6s\tremaining: 4.8s\n",
      "933:\ttotal: 1m 6s\tremaining: 4.73s\n",
      "934:\ttotal: 1m 6s\tremaining: 4.66s\n",
      "935:\ttotal: 1m 7s\tremaining: 4.58s\n",
      "936:\ttotal: 1m 7s\tremaining: 4.51s\n",
      "937:\ttotal: 1m 7s\tremaining: 4.44s\n",
      "938:\ttotal: 1m 7s\tremaining: 4.37s\n",
      "939:\ttotal: 1m 7s\tremaining: 4.3s\n",
      "940:\ttotal: 1m 7s\tremaining: 4.23s\n",
      "941:\ttotal: 1m 7s\tremaining: 4.15s\n",
      "942:\ttotal: 1m 7s\tremaining: 4.08s\n",
      "943:\ttotal: 1m 7s\tremaining: 4.01s\n",
      "944:\ttotal: 1m 7s\tremaining: 3.94s\n",
      "945:\ttotal: 1m 7s\tremaining: 3.87s\n",
      "946:\ttotal: 1m 7s\tremaining: 3.79s\n",
      "947:\ttotal: 1m 7s\tremaining: 3.72s\n",
      "948:\ttotal: 1m 7s\tremaining: 3.65s\n",
      "949:\ttotal: 1m 7s\tremaining: 3.58s\n",
      "950:\ttotal: 1m 8s\tremaining: 3.5s\n",
      "951:\ttotal: 1m 8s\tremaining: 3.43s\n",
      "952:\ttotal: 1m 8s\tremaining: 3.36s\n",
      "953:\ttotal: 1m 8s\tremaining: 3.29s\n",
      "954:\ttotal: 1m 8s\tremaining: 3.22s\n",
      "955:\ttotal: 1m 8s\tremaining: 3.15s\n",
      "956:\ttotal: 1m 8s\tremaining: 3.08s\n",
      "957:\ttotal: 1m 8s\tremaining: 3s\n",
      "958:\ttotal: 1m 8s\tremaining: 2.93s\n",
      "959:\ttotal: 1m 8s\tremaining: 2.86s\n",
      "960:\ttotal: 1m 8s\tremaining: 2.79s\n",
      "961:\ttotal: 1m 8s\tremaining: 2.71s\n",
      "962:\ttotal: 1m 8s\tremaining: 2.64s\n",
      "963:\ttotal: 1m 8s\tremaining: 2.57s\n",
      "964:\ttotal: 1m 8s\tremaining: 2.5s\n",
      "965:\ttotal: 1m 9s\tremaining: 2.43s\n",
      "966:\ttotal: 1m 9s\tremaining: 2.36s\n",
      "967:\ttotal: 1m 9s\tremaining: 2.29s\n",
      "968:\ttotal: 1m 9s\tremaining: 2.21s\n",
      "969:\ttotal: 1m 9s\tremaining: 2.14s\n",
      "970:\ttotal: 1m 9s\tremaining: 2.07s\n",
      "971:\ttotal: 1m 9s\tremaining: 2s\n",
      "972:\ttotal: 1m 9s\tremaining: 1.93s\n",
      "973:\ttotal: 1m 9s\tremaining: 1.86s\n",
      "974:\ttotal: 1m 9s\tremaining: 1.78s\n",
      "975:\ttotal: 1m 9s\tremaining: 1.71s\n",
      "976:\ttotal: 1m 9s\tremaining: 1.64s\n",
      "977:\ttotal: 1m 9s\tremaining: 1.57s\n",
      "978:\ttotal: 1m 9s\tremaining: 1.5s\n",
      "979:\ttotal: 1m 9s\tremaining: 1.43s\n",
      "980:\ttotal: 1m 10s\tremaining: 1.36s\n",
      "981:\ttotal: 1m 10s\tremaining: 1.28s\n",
      "982:\ttotal: 1m 10s\tremaining: 1.21s\n",
      "983:\ttotal: 1m 10s\tremaining: 1.14s\n",
      "984:\ttotal: 1m 10s\tremaining: 1.07s\n",
      "985:\ttotal: 1m 10s\tremaining: 999ms\n",
      "986:\ttotal: 1m 10s\tremaining: 928ms\n",
      "987:\ttotal: 1m 10s\tremaining: 857ms\n",
      "988:\ttotal: 1m 10s\tremaining: 785ms\n",
      "989:\ttotal: 1m 10s\tremaining: 714ms\n",
      "990:\ttotal: 1m 10s\tremaining: 642ms\n",
      "991:\ttotal: 1m 10s\tremaining: 571ms\n",
      "992:\ttotal: 1m 10s\tremaining: 499ms\n",
      "993:\ttotal: 1m 10s\tremaining: 428ms\n",
      "994:\ttotal: 1m 10s\tremaining: 356ms\n",
      "995:\ttotal: 1m 10s\tremaining: 285ms\n",
      "996:\ttotal: 1m 11s\tremaining: 214ms\n",
      "997:\ttotal: 1m 11s\tremaining: 142ms\n",
      "998:\ttotal: 1m 11s\tremaining: 71.2ms\n",
      "999:\ttotal: 1m 11s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fa1f12aa280>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "033622ae",
   "metadata": {
    "cellId": "yn36btexx6bzlo71f8xgno"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "predict = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "328382ec",
   "metadata": {
    "cellId": "tizdkm4pihs4wl5ag4rx9s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9322061002818867"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "rauc(list(y_test), list(predict), multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3f03fef7",
   "metadata": {
    "cellId": "vc9e2ftu1xd7idaif2br8x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-08 23:57:55,626]\u001b[0m A new study created in memory with name: no-name-b92a9dec-8596-4aee-8268-e61db21c4ed0\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:57:57,480]\u001b[0m Trial 0 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.09985752792364147, 'depth': 7, 'l2_leaf_reg': 120.7053060762596, 'bagging_temperature': 50.607511082516766, 'random_strength': 2.301199387253877}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:57:59,625]\u001b[0m Trial 1 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.00023398593722044245, 'depth': 3, 'l2_leaf_reg': 539.8801848500551, 'bagging_temperature': 55.723714957893925, 'random_strength': 3.4845325581323667}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:58:48,992]\u001b[0m Trial 2 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0570311974021931, 'depth': 5, 'l2_leaf_reg': 581.2795550136428, 'bagging_temperature': 17.393007255898915, 'random_strength': 6.033412451552875}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:58:50,643]\u001b[0m Trial 3 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.033106043822349565, 'depth': 2, 'l2_leaf_reg': 1.1638044227011577, 'bagging_temperature': 59.25999657365284, 'random_strength': 7.259885525112739}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:58:52,524]\u001b[0m Trial 4 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06805757448942948, 'depth': 3, 'l2_leaf_reg': 946.912631041953, 'bagging_temperature': 94.77594106132968, 'random_strength': 8.769756529032536}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-08 23:58:54,457]\u001b[0m Trial 5 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07091236203904745, 'depth': 6, 'l2_leaf_reg': 854.5958643939499, 'bagging_temperature': 82.59560239789025, 'random_strength': 4.76218791736986}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:01:57,244]\u001b[0m Trial 6 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.012092074448936925, 'depth': 8, 'l2_leaf_reg': 650.8603731890887, 'bagging_temperature': 29.72792277511202, 'random_strength': 9.960968077128262}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:23,684]\u001b[0m Trial 7 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06936398125186069, 'depth': 2, 'l2_leaf_reg': 837.9895864827055, 'bagging_temperature': 2.511120665387581, 'random_strength': 8.182124173755644}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:25,803]\u001b[0m Trial 8 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.03243703331973018, 'depth': 6, 'l2_leaf_reg': 227.41300354642442, 'bagging_temperature': 94.78139020004068, 'random_strength': 6.202167302575031}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:27,733]\u001b[0m Trial 9 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05864171606183406, 'depth': 7, 'l2_leaf_reg': 469.1871081106597, 'bagging_temperature': 89.31775868429114, 'random_strength': 1.9505560829296271}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:29,996]\u001b[0m Trial 10 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.09053693401538812, 'depth': 8, 'l2_leaf_reg': 13.408845930938128, 'bagging_temperature': 43.18103259446088, 'random_strength': 1.2319195306879234}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:31,573]\u001b[0m Trial 11 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0983865544230413, 'depth': 4, 'l2_leaf_reg': 354.91799613925593, 'bagging_temperature': 61.885523262836315, 'random_strength': 3.3414248407057148}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:34,196]\u001b[0m Trial 12 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.002981387783201568, 'depth': 4, 'l2_leaf_reg': 245.91399602999184, 'bagging_temperature': 47.34699016227841, 'random_strength': 0.2657115271458581}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:02:35,940]\u001b[0m Trial 13 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.03825931259097363, 'depth': 4, 'l2_leaf_reg': 473.54517961653045, 'bagging_temperature': 71.43198784891752, 'random_strength': 3.6049932296309946}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:35,719]\u001b[0m Trial 14 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.015997278549491276, 'depth': 6, 'l2_leaf_reg': 665.7852489924737, 'bagging_temperature': 37.8946898638968, 'random_strength': 2.654154073755089}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:38,188]\u001b[0m Trial 15 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0448562268742416, 'depth': 7, 'l2_leaf_reg': 148.30059950152332, 'bagging_temperature': 57.822807686657356, 'random_strength': 4.278028807683317}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:40,224]\u001b[0m Trial 16 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.023772709314271043, 'depth': 3, 'l2_leaf_reg': 345.65228488254826, 'bagging_temperature': 72.06779353406438, 'random_strength': 2.2901373591656498}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:41,767]\u001b[0m Trial 17 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.08176255693560684, 'depth': 5, 'l2_leaf_reg': 372.9536160124616, 'bagging_temperature': 53.802299280819305, 'random_strength': 3.790418550473626}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:48,581]\u001b[0m Trial 18 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0033670248902709808, 'depth': 3, 'l2_leaf_reg': 561.1194041105847, 'bagging_temperature': 34.13513987650998, 'random_strength': 0.08859318528554638}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:50,359]\u001b[0m Trial 19 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04889946829295141, 'depth': 7, 'l2_leaf_reg': 127.40596550253787, 'bagging_temperature': 47.12041295872756, 'random_strength': 5.29497435791966}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:03:52,369]\u001b[0m Trial 20 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.08285670373734864, 'depth': 5, 'l2_leaf_reg': 449.2931939726709, 'bagging_temperature': 67.12820563049773, 'random_strength': 2.8359783430827687}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:04:49,191]\u001b[0m Trial 21 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05644413332776301, 'depth': 5, 'l2_leaf_reg': 625.8749941467149, 'bagging_temperature': 25.578735079386565, 'random_strength': 5.474367325481712}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:05:29,224]\u001b[0m Trial 22 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04253545324706212, 'depth': 4, 'l2_leaf_reg': 562.2177416009175, 'bagging_temperature': 20.59790441681097, 'random_strength': 4.306845288548668}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:05:31,366]\u001b[0m Trial 23 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0551474991185702, 'depth': 6, 'l2_leaf_reg': 695.8516634370103, 'bagging_temperature': 48.7374629008577, 'random_strength': 6.219629953367303}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:05:42,473]\u001b[0m Trial 24 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.027936026070318573, 'depth': 3, 'l2_leaf_reg': 528.922538996035, 'bagging_temperature': 39.596147625789555, 'random_strength': 1.6639852580556806}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:07:52,249]\u001b[0m Trial 25 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.019886943621628985, 'depth': 7, 'l2_leaf_reg': 719.2148098217904, 'bagging_temperature': 11.213251020640314, 'random_strength': 3.2035716976549344}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:07:54,132]\u001b[0m Trial 26 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.036983746454183966, 'depth': 8, 'l2_leaf_reg': 581.1645497624927, 'bagging_temperature': 52.1623983984586, 'random_strength': 4.090831239411027}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:08:25,937]\u001b[0m Trial 27 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.09829518120966706, 'depth': 2, 'l2_leaf_reg': 429.9244829906655, 'bagging_temperature': 19.61030621261743, 'random_strength': 2.7575245497718623}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:30,132]\u001b[0m Trial 28 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.026492793401326754, 'depth': 5, 'l2_leaf_reg': 503.7210624115037, 'bagging_temperature': 35.06278950195633, 'random_strength': 4.759969677935274}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:32,230]\u001b[0m Trial 29 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.030829093351535088, 'depth': 2, 'l2_leaf_reg': 758.8889617005069, 'bagging_temperature': 58.98974289673782, 'random_strength': 6.901682786954989}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:34,214]\u001b[0m Trial 30 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06227496836949558, 'depth': 5, 'l2_leaf_reg': 613.9494771270331, 'bagging_temperature': 44.18412430976016, 'random_strength': 1.1833842113037587}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:36,503]\u001b[0m Trial 31 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.012067120576461142, 'depth': 2, 'l2_leaf_reg': 19.174808163924467, 'bagging_temperature': 54.3830359915426, 'random_strength': 7.151745198176587}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:38,371]\u001b[0m Trial 32 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.051318811629821916, 'depth': 3, 'l2_leaf_reg': 69.85265140256276, 'bagging_temperature': 63.248799866557526, 'random_strength': 7.38241106464781}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:46,602]\u001b[0m Trial 33 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06405195402829553, 'depth': 2, 'l2_leaf_reg': 87.57389605739226, 'bagging_temperature': 41.6771714923092, 'random_strength': 5.7309662550528415}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:48,705]\u001b[0m Trial 34 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07244209823842405, 'depth': 3, 'l2_leaf_reg': 952.2642679007417, 'bagging_temperature': 75.86996938631951, 'random_strength': 8.296867101087425}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:50,740]\u001b[0m Trial 35 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07609450328434283, 'depth': 2, 'l2_leaf_reg': 196.58183371581987, 'bagging_temperature': 55.87612686980809, 'random_strength': 9.973973623801884}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:09:52,918]\u001b[0m Trial 36 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.008849159887602279, 'depth': 4, 'l2_leaf_reg': 3.155400738326591, 'bagging_temperature': 64.47056117539293, 'random_strength': 9.251266889443656}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:13,816]\u001b[0m Trial 37 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.035624797841330096, 'depth': 6, 'l2_leaf_reg': 270.9642567542465, 'bagging_temperature': 1.6597059024299057, 'random_strength': 4.753666570670531}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:15,553]\u001b[0m Trial 38 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.021077895808373125, 'depth': 8, 'l2_leaf_reg': 59.34535622424448, 'bagging_temperature': 80.81389749440498, 'random_strength': 6.163550690326622}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:17,341]\u001b[0m Trial 39 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06359475697536754, 'depth': 6, 'l2_leaf_reg': 299.03786096883596, 'bagging_temperature': 50.28318102372768, 'random_strength': 7.677201849977387}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:19,309]\u001b[0m Trial 40 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0010657424171153715, 'depth': 4, 'l2_leaf_reg': 186.33386665318596, 'bagging_temperature': 59.6014323022237, 'random_strength': 6.434324158350762}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:20,967]\u001b[0m Trial 41 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06810203952968669, 'depth': 3, 'l2_leaf_reg': 808.8115377871591, 'bagging_temperature': 84.87650877566983, 'random_strength': 8.530976049410082}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:22,570]\u001b[0m Trial 42 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.046174979205395525, 'depth': 2, 'l2_leaf_reg': 907.1987692888988, 'bagging_temperature': 97.48276389987902, 'random_strength': 8.899563581601253}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:24,758]\u001b[0m Trial 43 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04186807914395479, 'depth': 3, 'l2_leaf_reg': 993.643266016094, 'bagging_temperature': 88.46182896010698, 'random_strength': 7.807661966015722}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:26,757]\u001b[0m Trial 44 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.051322710548231246, 'depth': 4, 'l2_leaf_reg': 412.1007546106779, 'bagging_temperature': 92.70791809956935, 'random_strength': 6.755115575674948}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:28,856]\u001b[0m Trial 45 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.03168452509888293, 'depth': 7, 'l2_leaf_reg': 492.76863964788635, 'bagging_temperature': 45.79263461386889, 'random_strength': 7.737878481724632}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:31,225]\u001b[0m Trial 46 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.057913386815968484, 'depth': 3, 'l2_leaf_reg': 877.8654890807097, 'bagging_temperature': 66.27501837640258, 'random_strength': 7.273651605632939}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:33,090]\u001b[0m Trial 47 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.09135559448180272, 'depth': 2, 'l2_leaf_reg': 662.8037189300594, 'bagging_temperature': 71.56614516850922, 'random_strength': 9.500209955848238}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:34,999]\u001b[0m Trial 48 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04012572799367767, 'depth': 3, 'l2_leaf_reg': 814.6902916409576, 'bagging_temperature': 77.79423922426938, 'random_strength': 3.4336921638735203}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:11:36,913]\u001b[0m Trial 49 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.046156293276143665, 'depth': 5, 'l2_leaf_reg': 336.73370018613934, 'bagging_temperature': 50.37167954855289, 'random_strength': 8.307320073971127}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:20,130]\u001b[0m Trial 50 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.017381273818411067, 'depth': 4, 'l2_leaf_reg': 122.22936039868794, 'bagging_temperature': 31.950433604241816, 'random_strength': 2.292606222968382}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:22,239]\u001b[0m Trial 51 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07680253431816363, 'depth': 6, 'l2_leaf_reg': 885.4522276343429, 'bagging_temperature': 99.20583988822666, 'random_strength': 5.09001931981425}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:24,503]\u001b[0m Trial 52 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06731260528625785, 'depth': 7, 'l2_leaf_reg': 770.1791743280444, 'bagging_temperature': 94.22691251856324, 'random_strength': 5.4481180192140775}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:26,215]\u001b[0m Trial 53 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06124107468261013, 'depth': 7, 'l2_leaf_reg': 619.7646348530608, 'bagging_temperature': 60.46952905456877, 'random_strength': 3.956475668348332}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:27,957]\u001b[0m Trial 54 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05326965110026048, 'depth': 8, 'l2_leaf_reg': 527.647416010349, 'bagging_temperature': 67.7814490316968, 'random_strength': 5.9059542273153465}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:30,406]\u001b[0m Trial 55 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05500719066215361, 'depth': 6, 'l2_leaf_reg': 394.1412241980013, 'bagging_temperature': 54.621881253424924, 'random_strength': 4.474339633930516}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:32,315]\u001b[0m Trial 56 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0594193865046857, 'depth': 5, 'l2_leaf_reg': 475.4945876011983, 'bagging_temperature': 41.69221819050644, 'random_strength': 3.56339079965647}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:34,016]\u001b[0m Trial 57 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05829600619749466, 'depth': 7, 'l2_leaf_reg': 454.91441167873904, 'bagging_temperature': 47.407932810197295, 'random_strength': 3.0355788427483232}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:36,493]\u001b[0m Trial 58 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.08324773195712343, 'depth': 6, 'l2_leaf_reg': 682.6572054968348, 'bagging_temperature': 68.90292122696802, 'random_strength': 3.719646181012328}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:38,347]\u001b[0m Trial 59 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06566918136049578, 'depth': 2, 'l2_leaf_reg': 581.8002603768075, 'bagging_temperature': 61.511005024877115, 'random_strength': 4.9494185112325875}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:40,555]\u001b[0m Trial 60 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07030476580972347, 'depth': 3, 'l2_leaf_reg': 728.9154897526716, 'bagging_temperature': 52.10742625780063, 'random_strength': 6.649141812715964}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:12:42,394]\u001b[0m Trial 61 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.006999801387615093, 'depth': 8, 'l2_leaf_reg': 640.0951191967597, 'bagging_temperature': 57.57683275119406, 'random_strength': 8.978015129392661}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:13:32,264]\u001b[0m Trial 62 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.014061846915850927, 'depth': 8, 'l2_leaf_reg': 565.4833628012344, 'bagging_temperature': 8.613463695761233, 'random_strength': 9.886247755405831}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:15:10,213]\u001b[0m Trial 63 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.004090340889985713, 'depth': 8, 'l2_leaf_reg': 529.1828650045796, 'bagging_temperature': 38.42453185765437, 'random_strength': 5.676974126229257}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:15:12,139]\u001b[0m Trial 64 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0004120074649082752, 'depth': 7, 'l2_leaf_reg': 40.59775780372069, 'bagging_temperature': 44.792649830946374, 'random_strength': 9.472399989644392}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:15:13,921]\u001b[0m Trial 65 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.023477109657836823, 'depth': 5, 'l2_leaf_reg': 99.30410516137482, 'bagging_temperature': 89.82937426614748, 'random_strength': 8.657478486532298}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:16:34,975]\u001b[0m Trial 66 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.009501682175035196, 'depth': 6, 'l2_leaf_reg': 594.3573633426763, 'bagging_temperature': 30.117617179558067, 'random_strength': 4.230775422556976}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:16:37,226]\u001b[0m Trial 67 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07062980065141605, 'depth': 7, 'l2_leaf_reg': 4.246321127756346, 'bagging_temperature': 63.818259260910025, 'random_strength': 5.26899254341186}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:16:38,994]\u001b[0m Trial 68 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04985712462010576, 'depth': 6, 'l2_leaf_reg': 650.6187890297736, 'bagging_temperature': 56.470912679281916, 'random_strength': 7.010640495271774}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:16:40,833]\u001b[0m Trial 69 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.061557333172732334, 'depth': 4, 'l2_leaf_reg': 436.2011120919458, 'bagging_temperature': 48.34971144602665, 'random_strength': 3.1360802256193065}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:17:03,250]\u001b[0m Trial 70 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.034013446282264354, 'depth': 2, 'l2_leaf_reg': 686.8790796785099, 'bagging_temperature': 23.141966594911274, 'random_strength': 4.531345218224937}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:17:23,749]\u001b[0m Trial 71 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.038863592414220265, 'depth': 2, 'l2_leaf_reg': 598.6775154737848, 'bagging_temperature': 26.09387254358463, 'random_strength': 8.102811721620316}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:17:48,323]\u001b[0m Trial 72 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07419077836922129, 'depth': 2, 'l2_leaf_reg': 41.96962945027283, 'bagging_temperature': 17.028082839044167, 'random_strength': 7.4352433424946875}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:18:25,128]\u001b[0m Trial 73 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.08002978998096044, 'depth': 3, 'l2_leaf_reg': 631.6683167805396, 'bagging_temperature': 35.02042922379486, 'random_strength': 8.06786687193205}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:18:45,424]\u001b[0m Trial 74 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06548213854285755, 'depth': 3, 'l2_leaf_reg': 541.0461935646175, 'bagging_temperature': 40.42760643200257, 'random_strength': 8.658624691572523}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:19:13,861]\u001b[0m Trial 75 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.09842576436818448, 'depth': 2, 'l2_leaf_reg': 832.277712225681, 'bagging_temperature': 1.0207924129117671, 'random_strength': 3.980641197698246}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:19:15,636]\u001b[0m Trial 76 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.028082108816450527, 'depth': 8, 'l2_leaf_reg': 935.4908547921899, 'bagging_temperature': 52.829809480596424, 'random_strength': 6.303130771983709}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:19:17,645]\u001b[0m Trial 77 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06767031210813733, 'depth': 2, 'l2_leaf_reg': 842.4958754955122, 'bagging_temperature': 84.51355732251325, 'random_strength': 2.580159009316233}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:19:45,358]\u001b[0m Trial 78 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0555671029068773, 'depth': 3, 'l2_leaf_reg': 163.71645455821098, 'bagging_temperature': 14.109393020069747, 'random_strength': 9.083225766659519}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:20:20,710]\u001b[0m Trial 79 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.043882522586802586, 'depth': 4, 'l2_leaf_reg': 230.09966818365328, 'bagging_temperature': 4.569258906062958, 'random_strength': 6.554519451549131}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:20:23,185]\u001b[0m Trial 80 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06074554792746913, 'depth': 7, 'l2_leaf_reg': 726.2157864880762, 'bagging_temperature': 42.63764237304474, 'random_strength': 6.848617556042519}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:20:25,697]\u001b[0m Trial 81 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.047532201827032126, 'depth': 5, 'l2_leaf_reg': 111.4673698219889, 'bagging_temperature': 96.61185271639229, 'random_strength': 6.127994599998537}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:20:27,505]\u001b[0m Trial 82 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.015428496485626522, 'depth': 6, 'l2_leaf_reg': 141.05219869071132, 'bagging_temperature': 94.13823246443032, 'random_strength': 7.25682744940645}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:20:29,757]\u001b[0m Trial 83 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.020448679279306346, 'depth': 6, 'l2_leaf_reg': 78.34866750826662, 'bagging_temperature': 58.7173787232003, 'random_strength': 5.912733593584454}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:12,880]\u001b[0m Trial 84 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.042052787634795474, 'depth': 5, 'l2_leaf_reg': 997.2270941880242, 'bagging_temperature': 37.356794946915485, 'random_strength': 7.6652374750934635}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:14,758]\u001b[0m Trial 85 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05288932659103177, 'depth': 7, 'l2_leaf_reg': 778.7533463737286, 'bagging_temperature': 91.19520924928185, 'random_strength': 7.017302066903401}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:16,837]\u001b[0m Trial 86 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.011662405198918847, 'depth': 2, 'l2_leaf_reg': 68.6614244316938, 'bagging_temperature': 87.60473285601505, 'random_strength': 6.433429973516605}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:18,666]\u001b[0m Trial 87 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06349078649652813, 'depth': 4, 'l2_leaf_reg': 134.71683227771075, 'bagging_temperature': 74.2902555123696, 'random_strength': 3.448547570992126}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:20,550]\u001b[0m Trial 88 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.03777982714733967, 'depth': 8, 'l2_leaf_reg': 503.47328469342614, 'bagging_temperature': 79.25782375940584, 'random_strength': 6.743698088505516}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:22,492]\u001b[0m Trial 89 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.017273642264935864, 'depth': 7, 'l2_leaf_reg': 101.99732712837397, 'bagging_temperature': 96.13748870385913, 'random_strength': 7.463636096404117}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:24,948]\u001b[0m Trial 90 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05743137951112151, 'depth': 2, 'l2_leaf_reg': 862.4071516095611, 'bagging_temperature': 98.50529077151089, 'random_strength': 8.41517936968172}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:26,909]\u001b[0m Trial 91 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.04967081209334826, 'depth': 7, 'l2_leaf_reg': 358.7033782376266, 'bagging_temperature': 85.35834566837586, 'random_strength': 1.663389438430149}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:29,002]\u001b[0m Trial 92 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06016210384951475, 'depth': 6, 'l2_leaf_reg': 906.6291048797266, 'bagging_temperature': 90.81175489149095, 'random_strength': 7.9660455390982925}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:30,958]\u001b[0m Trial 93 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0051284720281494405, 'depth': 8, 'l2_leaf_reg': 553.2757155761462, 'bagging_temperature': 93.76378049541246, 'random_strength': 3.7337346906255537}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:33,026]\u001b[0m Trial 94 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.056604968631312626, 'depth': 7, 'l2_leaf_reg': 484.2795052466021, 'bagging_temperature': 99.70695813280308, 'random_strength': 2.007487296965915}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:35,272]\u001b[0m Trial 95 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.06407378717689745, 'depth': 3, 'l2_leaf_reg': 455.3959114835071, 'bagging_temperature': 45.442915128340886, 'random_strength': 8.71811110605048}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:37,087]\u001b[0m Trial 96 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.07016180913796541, 'depth': 5, 'l2_leaf_reg': 610.8523760837713, 'bagging_temperature': 50.52791686651191, 'random_strength': 2.829386098765131}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:38,636]\u001b[0m Trial 97 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.0014954389775129436, 'depth': 7, 'l2_leaf_reg': 663.6092144121379, 'bagging_temperature': 82.47623944293372, 'random_strength': 9.294221329490219}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:40,584]\u001b[0m Trial 98 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.044643951870819265, 'depth': 6, 'l2_leaf_reg': 701.625384213945, 'bagging_temperature': 55.58055124895757, 'random_strength': 5.29741423772278}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n",
      "\u001b[32m[I 2023-04-09 00:21:42,958]\u001b[0m Trial 99 finished with value: 0.9322061002818867 and parameters: {'learning_rate': 0.05908673109580036, 'depth': 6, 'l2_leaf_reg': 573.9087513686111, 'bagging_temperature': 87.9174221386416, 'random_strength': 8.24248656498751}. Best is trial 0 with value: 0.9322061002818867.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n",
    "        'depth': trial.suggest_int('depth', 2, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 1e3),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 100.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-5, 10),\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 50,\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'AUC',\n",
    "        'logging_level': 'Silent',\n",
    "        'task_type': 'CPU'\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(x_train, y_train, eval_set=(x_test, y_test))\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "    accuracy = rauc(list(y_test), list(predict), multi_class = 'ovr')\n",
    "    return accuracy\n",
    "# Создание объекта Study для байесовской оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "\n",
    "# Запуск байесовской оптимизации\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c20c2d6a",
   "metadata": {
    "cellId": "9ay51tx7guso94npr3fiw8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.09985752792364147,\n",
       " 'depth': 7,\n",
       " 'l2_leaf_reg': 120.7053060762596,\n",
       " 'bagging_temperature': 50.607511082516766,\n",
       " 'random_strength': 2.301199387253877}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4079e",
   "metadata": {
    "cellId": "249hnz3e0hnj3i013fg73",
    "execution_id": "ad7092df-d01b-4dfe-837d-a17c8223069b"
   },
   "source": [
    "### Model training+testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "cd8385e6",
   "metadata": {
    "cellId": "9uvkzi7zd9w3qst9tf6wuo"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "model_cat = CatBoostClassifier(loss_function='MultiClass', eval_metric='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ca7bca88",
   "metadata": {
    "cellId": "2ak7r8br7dkihwku8s51c"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "x_test = np.load('sent_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "291fa198",
   "metadata": {
    "cellId": "7xmfmto1pr91f5ghmtqkq2"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "x_train = np.load('sent_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1e583569",
   "metadata": {
    "cellId": "h8we0ly4lie7qe1hle2"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d37eebe9",
   "metadata": {
    "cellId": "4ud34yb7pucs9plfhb0fo8",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.092036\n",
      "0:\ttotal: 178ms\tremaining: 2m 57s\n",
      "1:\ttotal: 268ms\tremaining: 2m 13s\n",
      "2:\ttotal: 349ms\tremaining: 1m 55s\n",
      "3:\ttotal: 414ms\tremaining: 1m 43s\n",
      "4:\ttotal: 483ms\tremaining: 1m 36s\n",
      "5:\ttotal: 568ms\tremaining: 1m 34s\n",
      "6:\ttotal: 657ms\tremaining: 1m 33s\n",
      "7:\ttotal: 752ms\tremaining: 1m 33s\n",
      "8:\ttotal: 836ms\tremaining: 1m 32s\n",
      "9:\ttotal: 919ms\tremaining: 1m 30s\n",
      "10:\ttotal: 1s\tremaining: 1m 30s\n",
      "11:\ttotal: 1.09s\tremaining: 1m 29s\n",
      "12:\ttotal: 1.18s\tremaining: 1m 29s\n",
      "13:\ttotal: 1.27s\tremaining: 1m 29s\n",
      "14:\ttotal: 1.36s\tremaining: 1m 29s\n",
      "15:\ttotal: 1.45s\tremaining: 1m 29s\n",
      "16:\ttotal: 1.52s\tremaining: 1m 28s\n",
      "17:\ttotal: 1.59s\tremaining: 1m 26s\n",
      "18:\ttotal: 1.67s\tremaining: 1m 26s\n",
      "19:\ttotal: 1.75s\tremaining: 1m 25s\n",
      "20:\ttotal: 1.84s\tremaining: 1m 25s\n",
      "21:\ttotal: 1.93s\tremaining: 1m 25s\n",
      "22:\ttotal: 2.05s\tremaining: 1m 27s\n",
      "23:\ttotal: 2.15s\tremaining: 1m 27s\n",
      "24:\ttotal: 2.24s\tremaining: 1m 27s\n",
      "25:\ttotal: 2.33s\tremaining: 1m 27s\n",
      "26:\ttotal: 2.4s\tremaining: 1m 26s\n",
      "27:\ttotal: 2.48s\tremaining: 1m 26s\n",
      "28:\ttotal: 2.56s\tremaining: 1m 25s\n",
      "29:\ttotal: 2.64s\tremaining: 1m 25s\n",
      "30:\ttotal: 2.73s\tremaining: 1m 25s\n",
      "31:\ttotal: 2.82s\tremaining: 1m 25s\n",
      "32:\ttotal: 2.91s\tremaining: 1m 25s\n",
      "33:\ttotal: 3s\tremaining: 1m 25s\n",
      "34:\ttotal: 3.11s\tremaining: 1m 25s\n",
      "35:\ttotal: 3.19s\tremaining: 1m 25s\n",
      "36:\ttotal: 3.27s\tremaining: 1m 25s\n",
      "37:\ttotal: 3.35s\tremaining: 1m 24s\n",
      "38:\ttotal: 3.42s\tremaining: 1m 24s\n",
      "39:\ttotal: 3.49s\tremaining: 1m 23s\n",
      "40:\ttotal: 3.58s\tremaining: 1m 23s\n",
      "41:\ttotal: 3.66s\tremaining: 1m 23s\n",
      "42:\ttotal: 3.73s\tremaining: 1m 23s\n",
      "43:\ttotal: 3.82s\tremaining: 1m 22s\n",
      "44:\ttotal: 3.89s\tremaining: 1m 22s\n",
      "45:\ttotal: 3.97s\tremaining: 1m 22s\n",
      "46:\ttotal: 4.04s\tremaining: 1m 22s\n",
      "47:\ttotal: 4.12s\tremaining: 1m 21s\n",
      "48:\ttotal: 4.21s\tremaining: 1m 21s\n",
      "49:\ttotal: 4.29s\tremaining: 1m 21s\n",
      "50:\ttotal: 4.37s\tremaining: 1m 21s\n",
      "51:\ttotal: 4.45s\tremaining: 1m 21s\n",
      "52:\ttotal: 4.54s\tremaining: 1m 21s\n",
      "53:\ttotal: 4.62s\tremaining: 1m 20s\n",
      "54:\ttotal: 4.73s\tremaining: 1m 21s\n",
      "55:\ttotal: 4.82s\tremaining: 1m 21s\n",
      "56:\ttotal: 4.89s\tremaining: 1m 20s\n",
      "57:\ttotal: 4.97s\tremaining: 1m 20s\n",
      "58:\ttotal: 5.06s\tremaining: 1m 20s\n",
      "59:\ttotal: 5.13s\tremaining: 1m 20s\n",
      "60:\ttotal: 5.23s\tremaining: 1m 20s\n",
      "61:\ttotal: 5.33s\tremaining: 1m 20s\n",
      "62:\ttotal: 5.43s\tremaining: 1m 20s\n",
      "63:\ttotal: 5.51s\tremaining: 1m 20s\n",
      "64:\ttotal: 5.61s\tremaining: 1m 20s\n",
      "65:\ttotal: 5.7s\tremaining: 1m 20s\n",
      "66:\ttotal: 5.78s\tremaining: 1m 20s\n",
      "67:\ttotal: 5.86s\tremaining: 1m 20s\n",
      "68:\ttotal: 5.94s\tremaining: 1m 20s\n",
      "69:\ttotal: 6.03s\tremaining: 1m 20s\n",
      "70:\ttotal: 6.1s\tremaining: 1m 19s\n",
      "71:\ttotal: 6.17s\tremaining: 1m 19s\n",
      "72:\ttotal: 6.26s\tremaining: 1m 19s\n",
      "73:\ttotal: 6.35s\tremaining: 1m 19s\n",
      "74:\ttotal: 6.45s\tremaining: 1m 19s\n",
      "75:\ttotal: 6.55s\tremaining: 1m 19s\n",
      "76:\ttotal: 6.63s\tremaining: 1m 19s\n",
      "77:\ttotal: 6.7s\tremaining: 1m 19s\n",
      "78:\ttotal: 6.77s\tremaining: 1m 18s\n",
      "79:\ttotal: 6.84s\tremaining: 1m 18s\n",
      "80:\ttotal: 6.91s\tremaining: 1m 18s\n",
      "81:\ttotal: 6.99s\tremaining: 1m 18s\n",
      "82:\ttotal: 7.07s\tremaining: 1m 18s\n",
      "83:\ttotal: 7.16s\tremaining: 1m 18s\n",
      "84:\ttotal: 7.27s\tremaining: 1m 18s\n",
      "85:\ttotal: 7.36s\tremaining: 1m 18s\n",
      "86:\ttotal: 7.45s\tremaining: 1m 18s\n",
      "87:\ttotal: 7.55s\tremaining: 1m 18s\n",
      "88:\ttotal: 7.64s\tremaining: 1m 18s\n",
      "89:\ttotal: 7.72s\tremaining: 1m 18s\n",
      "90:\ttotal: 7.81s\tremaining: 1m 18s\n",
      "91:\ttotal: 7.89s\tremaining: 1m 17s\n",
      "92:\ttotal: 7.98s\tremaining: 1m 17s\n",
      "93:\ttotal: 8.05s\tremaining: 1m 17s\n",
      "94:\ttotal: 8.13s\tremaining: 1m 17s\n",
      "95:\ttotal: 8.22s\tremaining: 1m 17s\n",
      "96:\ttotal: 8.32s\tremaining: 1m 17s\n",
      "97:\ttotal: 8.4s\tremaining: 1m 17s\n",
      "98:\ttotal: 8.47s\tremaining: 1m 17s\n",
      "99:\ttotal: 8.55s\tremaining: 1m 16s\n",
      "100:\ttotal: 8.65s\tremaining: 1m 16s\n",
      "101:\ttotal: 8.74s\tremaining: 1m 16s\n",
      "102:\ttotal: 8.83s\tremaining: 1m 16s\n",
      "103:\ttotal: 8.92s\tremaining: 1m 16s\n",
      "104:\ttotal: 9s\tremaining: 1m 16s\n",
      "105:\ttotal: 9.06s\tremaining: 1m 16s\n",
      "106:\ttotal: 9.15s\tremaining: 1m 16s\n",
      "107:\ttotal: 9.25s\tremaining: 1m 16s\n",
      "108:\ttotal: 9.34s\tremaining: 1m 16s\n",
      "109:\ttotal: 9.44s\tremaining: 1m 16s\n",
      "110:\ttotal: 9.52s\tremaining: 1m 16s\n",
      "111:\ttotal: 9.6s\tremaining: 1m 16s\n",
      "112:\ttotal: 9.7s\tremaining: 1m 16s\n",
      "113:\ttotal: 9.79s\tremaining: 1m 16s\n",
      "114:\ttotal: 9.88s\tremaining: 1m 16s\n",
      "115:\ttotal: 9.97s\tremaining: 1m 15s\n",
      "116:\ttotal: 10.1s\tremaining: 1m 15s\n",
      "117:\ttotal: 10.1s\tremaining: 1m 15s\n",
      "118:\ttotal: 10.2s\tremaining: 1m 15s\n",
      "119:\ttotal: 10.3s\tremaining: 1m 15s\n",
      "120:\ttotal: 10.5s\tremaining: 1m 16s\n",
      "121:\ttotal: 10.6s\tremaining: 1m 15s\n",
      "122:\ttotal: 10.6s\tremaining: 1m 15s\n",
      "123:\ttotal: 10.7s\tremaining: 1m 15s\n",
      "124:\ttotal: 10.8s\tremaining: 1m 15s\n",
      "125:\ttotal: 10.9s\tremaining: 1m 15s\n",
      "126:\ttotal: 10.9s\tremaining: 1m 15s\n",
      "127:\ttotal: 11s\tremaining: 1m 15s\n",
      "128:\ttotal: 11.1s\tremaining: 1m 15s\n",
      "129:\ttotal: 11.2s\tremaining: 1m 14s\n",
      "130:\ttotal: 11.3s\tremaining: 1m 14s\n",
      "131:\ttotal: 11.4s\tremaining: 1m 14s\n",
      "132:\ttotal: 11.4s\tremaining: 1m 14s\n",
      "133:\ttotal: 11.5s\tremaining: 1m 14s\n",
      "134:\ttotal: 11.6s\tremaining: 1m 14s\n",
      "135:\ttotal: 11.7s\tremaining: 1m 14s\n",
      "136:\ttotal: 11.8s\tremaining: 1m 14s\n",
      "137:\ttotal: 11.9s\tremaining: 1m 14s\n",
      "138:\ttotal: 11.9s\tremaining: 1m 14s\n",
      "139:\ttotal: 12s\tremaining: 1m 13s\n",
      "140:\ttotal: 12.1s\tremaining: 1m 13s\n",
      "141:\ttotal: 12.2s\tremaining: 1m 13s\n",
      "142:\ttotal: 12.3s\tremaining: 1m 13s\n",
      "143:\ttotal: 12.3s\tremaining: 1m 13s\n",
      "144:\ttotal: 12.4s\tremaining: 1m 13s\n",
      "145:\ttotal: 12.5s\tremaining: 1m 13s\n",
      "146:\ttotal: 12.6s\tremaining: 1m 13s\n",
      "147:\ttotal: 12.7s\tremaining: 1m 13s\n",
      "148:\ttotal: 12.8s\tremaining: 1m 12s\n",
      "149:\ttotal: 12.9s\tremaining: 1m 12s\n",
      "150:\ttotal: 12.9s\tremaining: 1m 12s\n",
      "151:\ttotal: 13s\tremaining: 1m 12s\n",
      "152:\ttotal: 13.1s\tremaining: 1m 12s\n",
      "153:\ttotal: 13.1s\tremaining: 1m 12s\n",
      "154:\ttotal: 13.2s\tremaining: 1m 12s\n",
      "155:\ttotal: 13.3s\tremaining: 1m 11s\n",
      "156:\ttotal: 13.4s\tremaining: 1m 11s\n",
      "157:\ttotal: 13.5s\tremaining: 1m 11s\n",
      "158:\ttotal: 13.6s\tremaining: 1m 11s\n",
      "159:\ttotal: 13.7s\tremaining: 1m 11s\n",
      "160:\ttotal: 13.8s\tremaining: 1m 11s\n",
      "161:\ttotal: 13.9s\tremaining: 1m 11s\n",
      "162:\ttotal: 14s\tremaining: 1m 11s\n",
      "163:\ttotal: 14.1s\tremaining: 1m 11s\n",
      "164:\ttotal: 14.2s\tremaining: 1m 11s\n",
      "165:\ttotal: 14.2s\tremaining: 1m 11s\n",
      "166:\ttotal: 14.3s\tremaining: 1m 11s\n",
      "167:\ttotal: 14.4s\tremaining: 1m 11s\n",
      "168:\ttotal: 14.5s\tremaining: 1m 11s\n",
      "169:\ttotal: 14.6s\tremaining: 1m 11s\n",
      "170:\ttotal: 14.7s\tremaining: 1m 11s\n",
      "171:\ttotal: 14.8s\tremaining: 1m 11s\n",
      "172:\ttotal: 14.9s\tremaining: 1m 11s\n",
      "173:\ttotal: 15s\tremaining: 1m 11s\n",
      "174:\ttotal: 15s\tremaining: 1m 10s\n",
      "175:\ttotal: 15.1s\tremaining: 1m 10s\n",
      "176:\ttotal: 15.2s\tremaining: 1m 10s\n",
      "177:\ttotal: 15.3s\tremaining: 1m 10s\n",
      "178:\ttotal: 15.4s\tremaining: 1m 10s\n",
      "179:\ttotal: 15.5s\tremaining: 1m 10s\n",
      "180:\ttotal: 15.6s\tremaining: 1m 10s\n",
      "181:\ttotal: 15.7s\tremaining: 1m 10s\n",
      "182:\ttotal: 15.7s\tremaining: 1m 10s\n",
      "183:\ttotal: 15.8s\tremaining: 1m 10s\n",
      "184:\ttotal: 15.9s\tremaining: 1m 10s\n",
      "185:\ttotal: 16s\tremaining: 1m 9s\n",
      "186:\ttotal: 16.1s\tremaining: 1m 9s\n",
      "187:\ttotal: 16.1s\tremaining: 1m 9s\n",
      "188:\ttotal: 16.2s\tremaining: 1m 9s\n",
      "189:\ttotal: 16.3s\tremaining: 1m 9s\n",
      "190:\ttotal: 16.3s\tremaining: 1m 9s\n",
      "191:\ttotal: 16.4s\tremaining: 1m 9s\n",
      "192:\ttotal: 16.5s\tremaining: 1m 9s\n",
      "193:\ttotal: 16.6s\tremaining: 1m 8s\n",
      "194:\ttotal: 16.7s\tremaining: 1m 8s\n",
      "195:\ttotal: 16.8s\tremaining: 1m 8s\n",
      "196:\ttotal: 16.8s\tremaining: 1m 8s\n",
      "197:\ttotal: 16.9s\tremaining: 1m 8s\n",
      "198:\ttotal: 17s\tremaining: 1m 8s\n",
      "199:\ttotal: 17.1s\tremaining: 1m 8s\n",
      "200:\ttotal: 17.2s\tremaining: 1m 8s\n",
      "201:\ttotal: 17.2s\tremaining: 1m 8s\n",
      "202:\ttotal: 17.3s\tremaining: 1m 7s\n",
      "203:\ttotal: 17.4s\tremaining: 1m 7s\n",
      "204:\ttotal: 17.5s\tremaining: 1m 7s\n",
      "205:\ttotal: 17.6s\tremaining: 1m 7s\n",
      "206:\ttotal: 17.6s\tremaining: 1m 7s\n",
      "207:\ttotal: 17.7s\tremaining: 1m 7s\n",
      "208:\ttotal: 17.8s\tremaining: 1m 7s\n",
      "209:\ttotal: 17.9s\tremaining: 1m 7s\n",
      "210:\ttotal: 17.9s\tremaining: 1m 7s\n",
      "211:\ttotal: 18s\tremaining: 1m 7s\n",
      "212:\ttotal: 18.1s\tremaining: 1m 7s\n",
      "213:\ttotal: 18.2s\tremaining: 1m 6s\n",
      "214:\ttotal: 18.3s\tremaining: 1m 6s\n",
      "215:\ttotal: 18.4s\tremaining: 1m 6s\n",
      "216:\ttotal: 18.5s\tremaining: 1m 6s\n",
      "217:\ttotal: 18.6s\tremaining: 1m 6s\n",
      "218:\ttotal: 18.7s\tremaining: 1m 6s\n",
      "219:\ttotal: 18.7s\tremaining: 1m 6s\n",
      "220:\ttotal: 18.8s\tremaining: 1m 6s\n",
      "221:\ttotal: 18.9s\tremaining: 1m 6s\n",
      "222:\ttotal: 19s\tremaining: 1m 6s\n",
      "223:\ttotal: 19.1s\tremaining: 1m 6s\n",
      "224:\ttotal: 19.2s\tremaining: 1m 6s\n",
      "225:\ttotal: 19.3s\tremaining: 1m 6s\n",
      "226:\ttotal: 19.4s\tremaining: 1m 5s\n",
      "227:\ttotal: 19.5s\tremaining: 1m 5s\n",
      "228:\ttotal: 19.5s\tremaining: 1m 5s\n",
      "229:\ttotal: 19.6s\tremaining: 1m 5s\n",
      "230:\ttotal: 19.7s\tremaining: 1m 5s\n",
      "231:\ttotal: 19.8s\tremaining: 1m 5s\n",
      "232:\ttotal: 19.9s\tremaining: 1m 5s\n",
      "233:\ttotal: 20s\tremaining: 1m 5s\n",
      "234:\ttotal: 20s\tremaining: 1m 5s\n",
      "235:\ttotal: 20.1s\tremaining: 1m 5s\n",
      "236:\ttotal: 20.2s\tremaining: 1m 4s\n",
      "237:\ttotal: 20.3s\tremaining: 1m 4s\n",
      "238:\ttotal: 20.3s\tremaining: 1m 4s\n",
      "239:\ttotal: 20.4s\tremaining: 1m 4s\n",
      "240:\ttotal: 20.5s\tremaining: 1m 4s\n",
      "241:\ttotal: 20.6s\tremaining: 1m 4s\n",
      "242:\ttotal: 20.7s\tremaining: 1m 4s\n",
      "243:\ttotal: 20.8s\tremaining: 1m 4s\n",
      "244:\ttotal: 20.9s\tremaining: 1m 4s\n",
      "245:\ttotal: 21s\tremaining: 1m 4s\n",
      "246:\ttotal: 21s\tremaining: 1m 4s\n",
      "247:\ttotal: 21.1s\tremaining: 1m 4s\n",
      "248:\ttotal: 21.2s\tremaining: 1m 3s\n",
      "249:\ttotal: 21.3s\tremaining: 1m 3s\n",
      "250:\ttotal: 21.4s\tremaining: 1m 3s\n",
      "251:\ttotal: 21.5s\tremaining: 1m 3s\n",
      "252:\ttotal: 21.5s\tremaining: 1m 3s\n",
      "253:\ttotal: 21.6s\tremaining: 1m 3s\n",
      "254:\ttotal: 21.7s\tremaining: 1m 3s\n",
      "255:\ttotal: 21.8s\tremaining: 1m 3s\n",
      "256:\ttotal: 21.9s\tremaining: 1m 3s\n",
      "257:\ttotal: 21.9s\tremaining: 1m 3s\n",
      "258:\ttotal: 22s\tremaining: 1m 3s\n",
      "259:\ttotal: 22.1s\tremaining: 1m 2s\n",
      "260:\ttotal: 22.2s\tremaining: 1m 2s\n",
      "261:\ttotal: 22.3s\tremaining: 1m 2s\n",
      "262:\ttotal: 22.4s\tremaining: 1m 2s\n",
      "263:\ttotal: 22.4s\tremaining: 1m 2s\n",
      "264:\ttotal: 22.5s\tremaining: 1m 2s\n",
      "265:\ttotal: 22.6s\tremaining: 1m 2s\n",
      "266:\ttotal: 22.7s\tremaining: 1m 2s\n",
      "267:\ttotal: 22.8s\tremaining: 1m 2s\n",
      "268:\ttotal: 22.9s\tremaining: 1m 2s\n",
      "269:\ttotal: 23s\tremaining: 1m 2s\n",
      "270:\ttotal: 23.1s\tremaining: 1m 2s\n",
      "271:\ttotal: 23.2s\tremaining: 1m 1s\n",
      "272:\ttotal: 23.2s\tremaining: 1m 1s\n",
      "273:\ttotal: 23.3s\tremaining: 1m 1s\n",
      "274:\ttotal: 23.4s\tremaining: 1m 1s\n",
      "275:\ttotal: 23.5s\tremaining: 1m 1s\n",
      "276:\ttotal: 23.6s\tremaining: 1m 1s\n",
      "277:\ttotal: 23.7s\tremaining: 1m 1s\n",
      "278:\ttotal: 23.8s\tremaining: 1m 1s\n",
      "279:\ttotal: 23.9s\tremaining: 1m 1s\n",
      "280:\ttotal: 23.9s\tremaining: 1m 1s\n",
      "281:\ttotal: 24s\tremaining: 1m 1s\n",
      "282:\ttotal: 24.1s\tremaining: 1m 1s\n",
      "283:\ttotal: 24.2s\tremaining: 1m 1s\n",
      "284:\ttotal: 24.3s\tremaining: 1m\n",
      "285:\ttotal: 24.4s\tremaining: 1m\n",
      "286:\ttotal: 24.5s\tremaining: 1m\n",
      "287:\ttotal: 24.5s\tremaining: 1m\n",
      "288:\ttotal: 24.6s\tremaining: 1m\n",
      "289:\ttotal: 24.7s\tremaining: 1m\n",
      "290:\ttotal: 24.8s\tremaining: 1m\n",
      "291:\ttotal: 24.9s\tremaining: 1m\n",
      "292:\ttotal: 25s\tremaining: 1m\n",
      "293:\ttotal: 25s\tremaining: 1m\n",
      "294:\ttotal: 25.1s\tremaining: 1m\n",
      "295:\ttotal: 25.2s\tremaining: 60s\n",
      "296:\ttotal: 25.3s\tremaining: 59.9s\n",
      "297:\ttotal: 25.4s\tremaining: 59.8s\n",
      "298:\ttotal: 25.5s\tremaining: 59.7s\n",
      "299:\ttotal: 25.6s\tremaining: 59.6s\n",
      "300:\ttotal: 25.6s\tremaining: 59.6s\n",
      "301:\ttotal: 25.7s\tremaining: 59.4s\n",
      "302:\ttotal: 25.8s\tremaining: 59.4s\n",
      "303:\ttotal: 25.9s\tremaining: 59.3s\n",
      "304:\ttotal: 26s\tremaining: 59.2s\n",
      "305:\ttotal: 26.1s\tremaining: 59.1s\n",
      "306:\ttotal: 26.1s\tremaining: 59s\n",
      "307:\ttotal: 26.2s\tremaining: 58.9s\n",
      "308:\ttotal: 26.3s\tremaining: 58.8s\n",
      "309:\ttotal: 26.4s\tremaining: 58.7s\n",
      "310:\ttotal: 26.5s\tremaining: 58.7s\n",
      "311:\ttotal: 26.6s\tremaining: 58.6s\n",
      "312:\ttotal: 26.6s\tremaining: 58.5s\n",
      "313:\ttotal: 26.7s\tremaining: 58.4s\n",
      "314:\ttotal: 26.8s\tremaining: 58.3s\n",
      "315:\ttotal: 26.9s\tremaining: 58.3s\n",
      "316:\ttotal: 27s\tremaining: 58.3s\n",
      "317:\ttotal: 27.1s\tremaining: 58.2s\n",
      "318:\ttotal: 27.2s\tremaining: 58.2s\n",
      "319:\ttotal: 27.3s\tremaining: 58.1s\n",
      "320:\ttotal: 27.4s\tremaining: 58s\n",
      "321:\ttotal: 27.5s\tremaining: 57.9s\n",
      "322:\ttotal: 27.6s\tremaining: 57.8s\n",
      "323:\ttotal: 27.6s\tremaining: 57.7s\n",
      "324:\ttotal: 27.7s\tremaining: 57.6s\n",
      "325:\ttotal: 27.8s\tremaining: 57.5s\n",
      "326:\ttotal: 27.9s\tremaining: 57.4s\n",
      "327:\ttotal: 28s\tremaining: 57.3s\n",
      "328:\ttotal: 28s\tremaining: 57.2s\n",
      "329:\ttotal: 28.1s\tremaining: 57.1s\n",
      "330:\ttotal: 28.2s\tremaining: 57s\n",
      "331:\ttotal: 28.3s\tremaining: 56.9s\n",
      "332:\ttotal: 28.3s\tremaining: 56.8s\n",
      "333:\ttotal: 28.4s\tremaining: 56.6s\n",
      "334:\ttotal: 28.5s\tremaining: 56.5s\n",
      "335:\ttotal: 28.6s\tremaining: 56.5s\n",
      "336:\ttotal: 28.7s\tremaining: 56.4s\n",
      "337:\ttotal: 28.8s\tremaining: 56.3s\n",
      "338:\ttotal: 28.8s\tremaining: 56.2s\n",
      "339:\ttotal: 28.9s\tremaining: 56.2s\n",
      "340:\ttotal: 29s\tremaining: 56.1s\n",
      "341:\ttotal: 29.1s\tremaining: 56s\n",
      "342:\ttotal: 29.2s\tremaining: 55.9s\n",
      "343:\ttotal: 29.3s\tremaining: 55.8s\n",
      "344:\ttotal: 29.4s\tremaining: 55.8s\n",
      "345:\ttotal: 29.5s\tremaining: 55.7s\n",
      "346:\ttotal: 29.6s\tremaining: 55.6s\n",
      "347:\ttotal: 29.6s\tremaining: 55.5s\n",
      "348:\ttotal: 29.7s\tremaining: 55.4s\n",
      "349:\ttotal: 29.8s\tremaining: 55.3s\n",
      "350:\ttotal: 29.9s\tremaining: 55.3s\n",
      "351:\ttotal: 30s\tremaining: 55.2s\n",
      "352:\ttotal: 30.1s\tremaining: 55.1s\n",
      "353:\ttotal: 30.2s\tremaining: 55.1s\n",
      "354:\ttotal: 30.3s\tremaining: 55s\n",
      "355:\ttotal: 30.4s\tremaining: 54.9s\n",
      "356:\ttotal: 30.5s\tremaining: 54.9s\n",
      "357:\ttotal: 30.6s\tremaining: 54.8s\n",
      "358:\ttotal: 30.7s\tremaining: 54.7s\n",
      "359:\ttotal: 30.7s\tremaining: 54.7s\n",
      "360:\ttotal: 30.8s\tremaining: 54.6s\n",
      "361:\ttotal: 30.9s\tremaining: 54.4s\n",
      "362:\ttotal: 31s\tremaining: 54.3s\n",
      "363:\ttotal: 31s\tremaining: 54.2s\n",
      "364:\ttotal: 31.1s\tremaining: 54.2s\n",
      "365:\ttotal: 31.2s\tremaining: 54.1s\n",
      "366:\ttotal: 31.3s\tremaining: 54s\n",
      "367:\ttotal: 31.4s\tremaining: 53.9s\n",
      "368:\ttotal: 31.5s\tremaining: 53.8s\n",
      "369:\ttotal: 31.6s\tremaining: 53.7s\n",
      "370:\ttotal: 31.6s\tremaining: 53.7s\n",
      "371:\ttotal: 31.7s\tremaining: 53.6s\n",
      "372:\ttotal: 31.8s\tremaining: 53.4s\n",
      "373:\ttotal: 31.9s\tremaining: 53.3s\n",
      "374:\ttotal: 31.9s\tremaining: 53.2s\n",
      "375:\ttotal: 32s\tremaining: 53.1s\n",
      "376:\ttotal: 32.1s\tremaining: 53s\n",
      "377:\ttotal: 32.2s\tremaining: 53s\n",
      "378:\ttotal: 32.3s\tremaining: 52.9s\n",
      "379:\ttotal: 32.3s\tremaining: 52.8s\n",
      "380:\ttotal: 32.4s\tremaining: 52.7s\n",
      "381:\ttotal: 32.5s\tremaining: 52.6s\n",
      "382:\ttotal: 32.6s\tremaining: 52.6s\n",
      "383:\ttotal: 32.7s\tremaining: 52.5s\n",
      "384:\ttotal: 32.8s\tremaining: 52.4s\n",
      "385:\ttotal: 32.9s\tremaining: 52.3s\n",
      "386:\ttotal: 33s\tremaining: 52.3s\n",
      "387:\ttotal: 33.1s\tremaining: 52.2s\n",
      "388:\ttotal: 33.2s\tremaining: 52.2s\n",
      "389:\ttotal: 33.3s\tremaining: 52.1s\n",
      "390:\ttotal: 33.4s\tremaining: 52s\n",
      "391:\ttotal: 33.4s\tremaining: 51.9s\n",
      "392:\ttotal: 33.5s\tremaining: 51.8s\n",
      "393:\ttotal: 33.6s\tremaining: 51.7s\n",
      "394:\ttotal: 33.7s\tremaining: 51.6s\n",
      "395:\ttotal: 33.8s\tremaining: 51.5s\n",
      "396:\ttotal: 33.9s\tremaining: 51.4s\n",
      "397:\ttotal: 33.9s\tremaining: 51.3s\n",
      "398:\ttotal: 34s\tremaining: 51.3s\n",
      "399:\ttotal: 34.1s\tremaining: 51.2s\n",
      "400:\ttotal: 34.2s\tremaining: 51.1s\n",
      "401:\ttotal: 34.3s\tremaining: 51s\n",
      "402:\ttotal: 34.4s\tremaining: 51s\n",
      "403:\ttotal: 34.5s\tremaining: 50.9s\n",
      "404:\ttotal: 34.6s\tremaining: 50.8s\n",
      "405:\ttotal: 34.7s\tremaining: 50.7s\n",
      "406:\ttotal: 34.8s\tremaining: 50.6s\n",
      "407:\ttotal: 34.8s\tremaining: 50.6s\n",
      "408:\ttotal: 34.9s\tremaining: 50.5s\n",
      "409:\ttotal: 35s\tremaining: 50.4s\n",
      "410:\ttotal: 35.1s\tremaining: 50.2s\n",
      "411:\ttotal: 35.1s\tremaining: 50.1s\n",
      "412:\ttotal: 35.2s\tremaining: 50s\n",
      "413:\ttotal: 35.3s\tremaining: 50s\n",
      "414:\ttotal: 35.4s\tremaining: 49.9s\n",
      "415:\ttotal: 35.4s\tremaining: 49.7s\n",
      "416:\ttotal: 35.5s\tremaining: 49.6s\n",
      "417:\ttotal: 35.6s\tremaining: 49.5s\n",
      "418:\ttotal: 35.6s\tremaining: 49.4s\n",
      "419:\ttotal: 35.7s\tremaining: 49.3s\n",
      "420:\ttotal: 35.8s\tremaining: 49.2s\n",
      "421:\ttotal: 35.8s\tremaining: 49.1s\n",
      "422:\ttotal: 35.9s\tremaining: 49s\n",
      "423:\ttotal: 36s\tremaining: 48.9s\n",
      "424:\ttotal: 36.1s\tremaining: 48.8s\n",
      "425:\ttotal: 36.2s\tremaining: 48.7s\n",
      "426:\ttotal: 36.3s\tremaining: 48.7s\n",
      "427:\ttotal: 36.3s\tremaining: 48.6s\n",
      "428:\ttotal: 36.4s\tremaining: 48.5s\n",
      "429:\ttotal: 36.5s\tremaining: 48.4s\n",
      "430:\ttotal: 36.6s\tremaining: 48.3s\n",
      "431:\ttotal: 36.7s\tremaining: 48.2s\n",
      "432:\ttotal: 36.7s\tremaining: 48.1s\n",
      "433:\ttotal: 36.8s\tremaining: 48s\n",
      "434:\ttotal: 36.9s\tremaining: 47.9s\n",
      "435:\ttotal: 37s\tremaining: 47.8s\n",
      "436:\ttotal: 37s\tremaining: 47.7s\n",
      "437:\ttotal: 37.1s\tremaining: 47.6s\n",
      "438:\ttotal: 37.2s\tremaining: 47.5s\n",
      "439:\ttotal: 37.3s\tremaining: 47.5s\n",
      "440:\ttotal: 37.4s\tremaining: 47.4s\n",
      "441:\ttotal: 37.5s\tremaining: 47.3s\n",
      "442:\ttotal: 37.5s\tremaining: 47.2s\n",
      "443:\ttotal: 37.6s\tremaining: 47.1s\n",
      "444:\ttotal: 37.8s\tremaining: 47.1s\n",
      "445:\ttotal: 37.8s\tremaining: 47s\n",
      "446:\ttotal: 37.9s\tremaining: 46.9s\n",
      "447:\ttotal: 38s\tremaining: 46.9s\n",
      "448:\ttotal: 38.1s\tremaining: 46.8s\n",
      "449:\ttotal: 38.2s\tremaining: 46.7s\n",
      "450:\ttotal: 38.3s\tremaining: 46.7s\n",
      "451:\ttotal: 38.4s\tremaining: 46.6s\n",
      "452:\ttotal: 38.5s\tremaining: 46.5s\n",
      "453:\ttotal: 38.6s\tremaining: 46.4s\n",
      "454:\ttotal: 38.7s\tremaining: 46.3s\n",
      "455:\ttotal: 38.8s\tremaining: 46.2s\n",
      "456:\ttotal: 38.9s\tremaining: 46.2s\n",
      "457:\ttotal: 38.9s\tremaining: 46.1s\n",
      "458:\ttotal: 39s\tremaining: 46s\n",
      "459:\ttotal: 39.1s\tremaining: 45.9s\n",
      "460:\ttotal: 39.2s\tremaining: 45.8s\n",
      "461:\ttotal: 39.3s\tremaining: 45.8s\n",
      "462:\ttotal: 39.4s\tremaining: 45.7s\n",
      "463:\ttotal: 39.4s\tremaining: 45.6s\n",
      "464:\ttotal: 39.5s\tremaining: 45.4s\n",
      "465:\ttotal: 39.6s\tremaining: 45.4s\n",
      "466:\ttotal: 39.7s\tremaining: 45.3s\n",
      "467:\ttotal: 39.7s\tremaining: 45.2s\n",
      "468:\ttotal: 39.8s\tremaining: 45.1s\n",
      "469:\ttotal: 39.9s\tremaining: 45s\n",
      "470:\ttotal: 40s\tremaining: 44.9s\n",
      "471:\ttotal: 40.1s\tremaining: 44.8s\n",
      "472:\ttotal: 40.2s\tremaining: 44.8s\n",
      "473:\ttotal: 40.3s\tremaining: 44.7s\n",
      "474:\ttotal: 40.3s\tremaining: 44.6s\n",
      "475:\ttotal: 40.4s\tremaining: 44.5s\n",
      "476:\ttotal: 40.5s\tremaining: 44.4s\n",
      "477:\ttotal: 40.6s\tremaining: 44.3s\n",
      "478:\ttotal: 40.7s\tremaining: 44.3s\n",
      "479:\ttotal: 40.8s\tremaining: 44.2s\n",
      "480:\ttotal: 40.9s\tremaining: 44.1s\n",
      "481:\ttotal: 40.9s\tremaining: 44s\n",
      "482:\ttotal: 41s\tremaining: 43.9s\n",
      "483:\ttotal: 41.1s\tremaining: 43.8s\n",
      "484:\ttotal: 41.1s\tremaining: 43.7s\n",
      "485:\ttotal: 41.2s\tremaining: 43.6s\n",
      "486:\ttotal: 41.3s\tremaining: 43.5s\n",
      "487:\ttotal: 41.4s\tremaining: 43.4s\n",
      "488:\ttotal: 41.5s\tremaining: 43.4s\n",
      "489:\ttotal: 41.6s\tremaining: 43.3s\n",
      "490:\ttotal: 41.7s\tremaining: 43.2s\n",
      "491:\ttotal: 41.7s\tremaining: 43.1s\n",
      "492:\ttotal: 41.8s\tremaining: 43s\n",
      "493:\ttotal: 41.9s\tremaining: 42.9s\n",
      "494:\ttotal: 42s\tremaining: 42.8s\n",
      "495:\ttotal: 42.1s\tremaining: 42.7s\n",
      "496:\ttotal: 42.1s\tremaining: 42.7s\n",
      "497:\ttotal: 42.2s\tremaining: 42.6s\n",
      "498:\ttotal: 42.3s\tremaining: 42.5s\n",
      "499:\ttotal: 42.4s\tremaining: 42.4s\n",
      "500:\ttotal: 42.5s\tremaining: 42.3s\n",
      "501:\ttotal: 42.6s\tremaining: 42.2s\n",
      "502:\ttotal: 42.7s\tremaining: 42.2s\n",
      "503:\ttotal: 42.8s\tremaining: 42.1s\n",
      "504:\ttotal: 42.8s\tremaining: 42s\n",
      "505:\ttotal: 42.9s\tremaining: 41.9s\n",
      "506:\ttotal: 43s\tremaining: 41.8s\n",
      "507:\ttotal: 43.1s\tremaining: 41.7s\n",
      "508:\ttotal: 43.2s\tremaining: 41.6s\n",
      "509:\ttotal: 43.3s\tremaining: 41.6s\n",
      "510:\ttotal: 43.4s\tremaining: 41.5s\n",
      "511:\ttotal: 43.4s\tremaining: 41.4s\n",
      "512:\ttotal: 43.5s\tremaining: 41.3s\n",
      "513:\ttotal: 43.6s\tremaining: 41.2s\n",
      "514:\ttotal: 43.7s\tremaining: 41.2s\n",
      "515:\ttotal: 43.8s\tremaining: 41.1s\n",
      "516:\ttotal: 43.9s\tremaining: 41s\n",
      "517:\ttotal: 44s\tremaining: 40.9s\n",
      "518:\ttotal: 44.1s\tremaining: 40.8s\n",
      "519:\ttotal: 44.1s\tremaining: 40.7s\n",
      "520:\ttotal: 44.2s\tremaining: 40.7s\n",
      "521:\ttotal: 44.3s\tremaining: 40.6s\n",
      "522:\ttotal: 44.4s\tremaining: 40.5s\n",
      "523:\ttotal: 44.4s\tremaining: 40.4s\n",
      "524:\ttotal: 44.5s\tremaining: 40.3s\n",
      "525:\ttotal: 44.6s\tremaining: 40.2s\n",
      "526:\ttotal: 44.7s\tremaining: 40.1s\n",
      "527:\ttotal: 44.8s\tremaining: 40s\n",
      "528:\ttotal: 44.8s\tremaining: 39.9s\n",
      "529:\ttotal: 44.9s\tremaining: 39.8s\n",
      "530:\ttotal: 45s\tremaining: 39.7s\n",
      "531:\ttotal: 45.1s\tremaining: 39.6s\n",
      "532:\ttotal: 45.2s\tremaining: 39.6s\n",
      "533:\ttotal: 45.2s\tremaining: 39.5s\n",
      "534:\ttotal: 45.3s\tremaining: 39.4s\n",
      "535:\ttotal: 45.4s\tremaining: 39.3s\n",
      "536:\ttotal: 45.5s\tremaining: 39.2s\n",
      "537:\ttotal: 45.5s\tremaining: 39.1s\n",
      "538:\ttotal: 45.6s\tremaining: 39s\n",
      "539:\ttotal: 45.7s\tremaining: 38.9s\n",
      "540:\ttotal: 45.8s\tremaining: 38.8s\n",
      "541:\ttotal: 45.9s\tremaining: 38.7s\n",
      "542:\ttotal: 45.9s\tremaining: 38.7s\n",
      "543:\ttotal: 46s\tremaining: 38.6s\n",
      "544:\ttotal: 46.1s\tremaining: 38.5s\n",
      "545:\ttotal: 46.2s\tremaining: 38.4s\n",
      "546:\ttotal: 46.3s\tremaining: 38.3s\n",
      "547:\ttotal: 46.4s\tremaining: 38.2s\n",
      "548:\ttotal: 46.4s\tremaining: 38.1s\n",
      "549:\ttotal: 46.5s\tremaining: 38.1s\n",
      "550:\ttotal: 46.6s\tremaining: 38s\n",
      "551:\ttotal: 46.7s\tremaining: 37.9s\n",
      "552:\ttotal: 46.8s\tremaining: 37.8s\n",
      "553:\ttotal: 46.9s\tremaining: 37.7s\n",
      "554:\ttotal: 47s\tremaining: 37.7s\n",
      "555:\ttotal: 47.1s\tremaining: 37.6s\n",
      "556:\ttotal: 47.2s\tremaining: 37.5s\n",
      "557:\ttotal: 47.2s\tremaining: 37.4s\n",
      "558:\ttotal: 47.3s\tremaining: 37.3s\n",
      "559:\ttotal: 47.4s\tremaining: 37.2s\n",
      "560:\ttotal: 47.5s\tremaining: 37.2s\n",
      "561:\ttotal: 47.6s\tremaining: 37.1s\n",
      "562:\ttotal: 47.7s\tremaining: 37s\n",
      "563:\ttotal: 47.7s\tremaining: 36.9s\n",
      "564:\ttotal: 47.8s\tremaining: 36.8s\n",
      "565:\ttotal: 47.9s\tremaining: 36.7s\n",
      "566:\ttotal: 48s\tremaining: 36.6s\n",
      "567:\ttotal: 48s\tremaining: 36.5s\n",
      "568:\ttotal: 48.1s\tremaining: 36.5s\n",
      "569:\ttotal: 48.2s\tremaining: 36.4s\n",
      "570:\ttotal: 48.3s\tremaining: 36.3s\n",
      "571:\ttotal: 48.4s\tremaining: 36.2s\n",
      "572:\ttotal: 48.5s\tremaining: 36.1s\n",
      "573:\ttotal: 48.6s\tremaining: 36s\n",
      "574:\ttotal: 48.6s\tremaining: 36s\n",
      "575:\ttotal: 48.7s\tremaining: 35.9s\n",
      "576:\ttotal: 48.8s\tremaining: 35.8s\n",
      "577:\ttotal: 48.8s\tremaining: 35.7s\n",
      "578:\ttotal: 48.9s\tremaining: 35.6s\n",
      "579:\ttotal: 49s\tremaining: 35.5s\n",
      "580:\ttotal: 49.1s\tremaining: 35.4s\n",
      "581:\ttotal: 49.2s\tremaining: 35.3s\n",
      "582:\ttotal: 49.3s\tremaining: 35.3s\n",
      "583:\ttotal: 49.4s\tremaining: 35.2s\n",
      "584:\ttotal: 49.5s\tremaining: 35.1s\n",
      "585:\ttotal: 49.6s\tremaining: 35s\n",
      "586:\ttotal: 49.7s\tremaining: 34.9s\n",
      "587:\ttotal: 49.7s\tremaining: 34.9s\n",
      "588:\ttotal: 49.8s\tremaining: 34.8s\n",
      "589:\ttotal: 49.9s\tremaining: 34.7s\n",
      "590:\ttotal: 50s\tremaining: 34.6s\n",
      "591:\ttotal: 50.1s\tremaining: 34.5s\n",
      "592:\ttotal: 50.2s\tremaining: 34.4s\n",
      "593:\ttotal: 50.2s\tremaining: 34.3s\n",
      "594:\ttotal: 50.3s\tremaining: 34.3s\n",
      "595:\ttotal: 50.4s\tremaining: 34.2s\n",
      "596:\ttotal: 50.5s\tremaining: 34.1s\n",
      "597:\ttotal: 50.5s\tremaining: 34s\n",
      "598:\ttotal: 50.6s\tremaining: 33.9s\n",
      "599:\ttotal: 50.7s\tremaining: 33.8s\n",
      "600:\ttotal: 50.8s\tremaining: 33.7s\n",
      "601:\ttotal: 50.8s\tremaining: 33.6s\n",
      "602:\ttotal: 50.9s\tremaining: 33.5s\n",
      "603:\ttotal: 51s\tremaining: 33.4s\n",
      "604:\ttotal: 51.1s\tremaining: 33.3s\n",
      "605:\ttotal: 51.1s\tremaining: 33.3s\n",
      "606:\ttotal: 51.2s\tremaining: 33.2s\n",
      "607:\ttotal: 51.3s\tremaining: 33.1s\n",
      "608:\ttotal: 51.4s\tremaining: 33s\n",
      "609:\ttotal: 51.4s\tremaining: 32.9s\n",
      "610:\ttotal: 51.5s\tremaining: 32.8s\n",
      "611:\ttotal: 51.6s\tremaining: 32.7s\n",
      "612:\ttotal: 51.7s\tremaining: 32.6s\n",
      "613:\ttotal: 51.8s\tremaining: 32.5s\n",
      "614:\ttotal: 51.8s\tremaining: 32.4s\n",
      "615:\ttotal: 51.9s\tremaining: 32.3s\n",
      "616:\ttotal: 52s\tremaining: 32.3s\n",
      "617:\ttotal: 52s\tremaining: 32.2s\n",
      "618:\ttotal: 52.1s\tremaining: 32.1s\n",
      "619:\ttotal: 52.2s\tremaining: 32s\n",
      "620:\ttotal: 52.3s\tremaining: 31.9s\n",
      "621:\ttotal: 52.4s\tremaining: 31.8s\n",
      "622:\ttotal: 52.4s\tremaining: 31.7s\n",
      "623:\ttotal: 52.5s\tremaining: 31.6s\n",
      "624:\ttotal: 52.6s\tremaining: 31.5s\n",
      "625:\ttotal: 52.7s\tremaining: 31.5s\n",
      "626:\ttotal: 52.7s\tremaining: 31.4s\n",
      "627:\ttotal: 52.8s\tremaining: 31.3s\n",
      "628:\ttotal: 52.9s\tremaining: 31.2s\n",
      "629:\ttotal: 53s\tremaining: 31.1s\n",
      "630:\ttotal: 53.1s\tremaining: 31s\n",
      "631:\ttotal: 53.2s\tremaining: 31s\n",
      "632:\ttotal: 53.3s\tremaining: 30.9s\n",
      "633:\ttotal: 53.3s\tremaining: 30.8s\n",
      "634:\ttotal: 53.4s\tremaining: 30.7s\n",
      "635:\ttotal: 53.5s\tremaining: 30.6s\n",
      "636:\ttotal: 53.5s\tremaining: 30.5s\n",
      "637:\ttotal: 53.6s\tremaining: 30.4s\n",
      "638:\ttotal: 53.7s\tremaining: 30.3s\n",
      "639:\ttotal: 53.8s\tremaining: 30.3s\n",
      "640:\ttotal: 53.9s\tremaining: 30.2s\n",
      "641:\ttotal: 54s\tremaining: 30.1s\n",
      "642:\ttotal: 54s\tremaining: 30s\n",
      "643:\ttotal: 54.1s\tremaining: 29.9s\n",
      "644:\ttotal: 54.2s\tremaining: 29.8s\n",
      "645:\ttotal: 54.2s\tremaining: 29.7s\n",
      "646:\ttotal: 54.3s\tremaining: 29.6s\n",
      "647:\ttotal: 54.4s\tremaining: 29.6s\n",
      "648:\ttotal: 54.5s\tremaining: 29.5s\n",
      "649:\ttotal: 54.5s\tremaining: 29.4s\n",
      "650:\ttotal: 54.6s\tremaining: 29.3s\n",
      "651:\ttotal: 54.7s\tremaining: 29.2s\n",
      "652:\ttotal: 54.8s\tremaining: 29.1s\n",
      "653:\ttotal: 54.9s\tremaining: 29s\n",
      "654:\ttotal: 55s\tremaining: 28.9s\n",
      "655:\ttotal: 55s\tremaining: 28.9s\n",
      "656:\ttotal: 55.1s\tremaining: 28.8s\n",
      "657:\ttotal: 55.2s\tremaining: 28.7s\n",
      "658:\ttotal: 55.3s\tremaining: 28.6s\n",
      "659:\ttotal: 55.4s\tremaining: 28.5s\n",
      "660:\ttotal: 55.5s\tremaining: 28.4s\n",
      "661:\ttotal: 55.5s\tremaining: 28.3s\n",
      "662:\ttotal: 55.6s\tremaining: 28.2s\n",
      "663:\ttotal: 55.6s\tremaining: 28.2s\n",
      "664:\ttotal: 55.7s\tremaining: 28.1s\n",
      "665:\ttotal: 55.8s\tremaining: 28s\n",
      "666:\ttotal: 55.9s\tremaining: 27.9s\n",
      "667:\ttotal: 56s\tremaining: 27.8s\n",
      "668:\ttotal: 56s\tremaining: 27.7s\n",
      "669:\ttotal: 56.1s\tremaining: 27.6s\n",
      "670:\ttotal: 56.2s\tremaining: 27.6s\n",
      "671:\ttotal: 56.3s\tremaining: 27.5s\n",
      "672:\ttotal: 56.4s\tremaining: 27.4s\n",
      "673:\ttotal: 56.4s\tremaining: 27.3s\n",
      "674:\ttotal: 56.5s\tremaining: 27.2s\n",
      "675:\ttotal: 56.6s\tremaining: 27.1s\n",
      "676:\ttotal: 56.6s\tremaining: 27s\n",
      "677:\ttotal: 56.7s\tremaining: 26.9s\n",
      "678:\ttotal: 56.8s\tremaining: 26.9s\n",
      "679:\ttotal: 56.9s\tremaining: 26.8s\n",
      "680:\ttotal: 57s\tremaining: 26.7s\n",
      "681:\ttotal: 57.1s\tremaining: 26.6s\n",
      "682:\ttotal: 57.2s\tremaining: 26.5s\n",
      "683:\ttotal: 57.3s\tremaining: 26.5s\n",
      "684:\ttotal: 57.3s\tremaining: 26.4s\n",
      "685:\ttotal: 57.4s\tremaining: 26.3s\n",
      "686:\ttotal: 57.5s\tremaining: 26.2s\n",
      "687:\ttotal: 57.6s\tremaining: 26.1s\n",
      "688:\ttotal: 57.6s\tremaining: 26s\n",
      "689:\ttotal: 57.7s\tremaining: 25.9s\n",
      "690:\ttotal: 57.8s\tremaining: 25.9s\n",
      "691:\ttotal: 57.9s\tremaining: 25.8s\n",
      "692:\ttotal: 58s\tremaining: 25.7s\n",
      "693:\ttotal: 58.1s\tremaining: 25.6s\n",
      "694:\ttotal: 58.2s\tremaining: 25.5s\n",
      "695:\ttotal: 58.3s\tremaining: 25.5s\n",
      "696:\ttotal: 58.4s\tremaining: 25.4s\n",
      "697:\ttotal: 58.5s\tremaining: 25.3s\n",
      "698:\ttotal: 58.6s\tremaining: 25.2s\n",
      "699:\ttotal: 58.6s\tremaining: 25.1s\n",
      "700:\ttotal: 58.7s\tremaining: 25.1s\n",
      "701:\ttotal: 58.8s\tremaining: 25s\n",
      "702:\ttotal: 58.9s\tremaining: 24.9s\n",
      "703:\ttotal: 59s\tremaining: 24.8s\n",
      "704:\ttotal: 59.1s\tremaining: 24.7s\n",
      "705:\ttotal: 59.2s\tremaining: 24.6s\n",
      "706:\ttotal: 59.3s\tremaining: 24.6s\n",
      "707:\ttotal: 59.3s\tremaining: 24.5s\n",
      "708:\ttotal: 59.4s\tremaining: 24.4s\n",
      "709:\ttotal: 59.5s\tremaining: 24.3s\n",
      "710:\ttotal: 59.6s\tremaining: 24.2s\n",
      "711:\ttotal: 59.7s\tremaining: 24.1s\n",
      "712:\ttotal: 59.8s\tremaining: 24.1s\n",
      "713:\ttotal: 59.9s\tremaining: 24s\n",
      "714:\ttotal: 59.9s\tremaining: 23.9s\n",
      "715:\ttotal: 1m\tremaining: 23.8s\n",
      "716:\ttotal: 1m\tremaining: 23.7s\n",
      "717:\ttotal: 1m\tremaining: 23.6s\n",
      "718:\ttotal: 1m\tremaining: 23.6s\n",
      "719:\ttotal: 1m\tremaining: 23.5s\n",
      "720:\ttotal: 1m\tremaining: 23.4s\n",
      "721:\ttotal: 1m\tremaining: 23.3s\n",
      "722:\ttotal: 1m\tremaining: 23.2s\n",
      "723:\ttotal: 1m\tremaining: 23.1s\n",
      "724:\ttotal: 1m\tremaining: 23s\n",
      "725:\ttotal: 1m\tremaining: 23s\n",
      "726:\ttotal: 1m\tremaining: 22.9s\n",
      "727:\ttotal: 1m\tremaining: 22.8s\n",
      "728:\ttotal: 1m 1s\tremaining: 22.7s\n",
      "729:\ttotal: 1m 1s\tremaining: 22.6s\n",
      "730:\ttotal: 1m 1s\tremaining: 22.5s\n",
      "731:\ttotal: 1m 1s\tremaining: 22.4s\n",
      "732:\ttotal: 1m 1s\tremaining: 22.3s\n",
      "733:\ttotal: 1m 1s\tremaining: 22.2s\n",
      "734:\ttotal: 1m 1s\tremaining: 22.2s\n",
      "735:\ttotal: 1m 1s\tremaining: 22.1s\n",
      "736:\ttotal: 1m 1s\tremaining: 22s\n",
      "737:\ttotal: 1m 1s\tremaining: 21.9s\n",
      "738:\ttotal: 1m 1s\tremaining: 21.8s\n",
      "739:\ttotal: 1m 1s\tremaining: 21.7s\n",
      "740:\ttotal: 1m 1s\tremaining: 21.6s\n",
      "741:\ttotal: 1m 1s\tremaining: 21.6s\n",
      "742:\ttotal: 1m 2s\tremaining: 21.5s\n",
      "743:\ttotal: 1m 2s\tremaining: 21.4s\n",
      "744:\ttotal: 1m 2s\tremaining: 21.3s\n",
      "745:\ttotal: 1m 2s\tremaining: 21.2s\n",
      "746:\ttotal: 1m 2s\tremaining: 21.1s\n",
      "747:\ttotal: 1m 2s\tremaining: 21s\n",
      "748:\ttotal: 1m 2s\tremaining: 20.9s\n",
      "749:\ttotal: 1m 2s\tremaining: 20.9s\n",
      "750:\ttotal: 1m 2s\tremaining: 20.8s\n",
      "751:\ttotal: 1m 2s\tremaining: 20.7s\n",
      "752:\ttotal: 1m 2s\tremaining: 20.6s\n",
      "753:\ttotal: 1m 2s\tremaining: 20.5s\n",
      "754:\ttotal: 1m 2s\tremaining: 20.4s\n",
      "755:\ttotal: 1m 2s\tremaining: 20.3s\n",
      "756:\ttotal: 1m 3s\tremaining: 20.2s\n",
      "757:\ttotal: 1m 3s\tremaining: 20.2s\n",
      "758:\ttotal: 1m 3s\tremaining: 20.1s\n",
      "759:\ttotal: 1m 3s\tremaining: 20s\n",
      "760:\ttotal: 1m 3s\tremaining: 19.9s\n",
      "761:\ttotal: 1m 3s\tremaining: 19.8s\n",
      "762:\ttotal: 1m 3s\tremaining: 19.7s\n",
      "763:\ttotal: 1m 3s\tremaining: 19.7s\n",
      "764:\ttotal: 1m 3s\tremaining: 19.6s\n",
      "765:\ttotal: 1m 3s\tremaining: 19.5s\n",
      "766:\ttotal: 1m 3s\tremaining: 19.4s\n",
      "767:\ttotal: 1m 3s\tremaining: 19.3s\n",
      "768:\ttotal: 1m 4s\tremaining: 19.2s\n",
      "769:\ttotal: 1m 4s\tremaining: 19.1s\n",
      "770:\ttotal: 1m 4s\tremaining: 19s\n",
      "771:\ttotal: 1m 4s\tremaining: 19s\n",
      "772:\ttotal: 1m 4s\tremaining: 18.9s\n",
      "773:\ttotal: 1m 4s\tremaining: 18.8s\n",
      "774:\ttotal: 1m 4s\tremaining: 18.7s\n",
      "775:\ttotal: 1m 4s\tremaining: 18.6s\n",
      "776:\ttotal: 1m 4s\tremaining: 18.5s\n",
      "777:\ttotal: 1m 4s\tremaining: 18.5s\n",
      "778:\ttotal: 1m 4s\tremaining: 18.4s\n",
      "779:\ttotal: 1m 4s\tremaining: 18.3s\n",
      "780:\ttotal: 1m 4s\tremaining: 18.2s\n",
      "781:\ttotal: 1m 4s\tremaining: 18.1s\n",
      "782:\ttotal: 1m 5s\tremaining: 18s\n",
      "783:\ttotal: 1m 5s\tremaining: 17.9s\n",
      "784:\ttotal: 1m 5s\tremaining: 17.9s\n",
      "785:\ttotal: 1m 5s\tremaining: 17.8s\n",
      "786:\ttotal: 1m 5s\tremaining: 17.7s\n",
      "787:\ttotal: 1m 5s\tremaining: 17.6s\n",
      "788:\ttotal: 1m 5s\tremaining: 17.5s\n",
      "789:\ttotal: 1m 5s\tremaining: 17.4s\n",
      "790:\ttotal: 1m 5s\tremaining: 17.4s\n",
      "791:\ttotal: 1m 5s\tremaining: 17.3s\n",
      "792:\ttotal: 1m 5s\tremaining: 17.2s\n",
      "793:\ttotal: 1m 5s\tremaining: 17.1s\n",
      "794:\ttotal: 1m 6s\tremaining: 17s\n",
      "795:\ttotal: 1m 6s\tremaining: 16.9s\n",
      "796:\ttotal: 1m 6s\tremaining: 16.9s\n",
      "797:\ttotal: 1m 6s\tremaining: 16.8s\n",
      "798:\ttotal: 1m 6s\tremaining: 16.7s\n",
      "799:\ttotal: 1m 6s\tremaining: 16.6s\n",
      "800:\ttotal: 1m 6s\tremaining: 16.5s\n",
      "801:\ttotal: 1m 6s\tremaining: 16.4s\n",
      "802:\ttotal: 1m 6s\tremaining: 16.3s\n",
      "803:\ttotal: 1m 6s\tremaining: 16.3s\n",
      "804:\ttotal: 1m 6s\tremaining: 16.2s\n",
      "805:\ttotal: 1m 6s\tremaining: 16.1s\n",
      "806:\ttotal: 1m 6s\tremaining: 16s\n",
      "807:\ttotal: 1m 6s\tremaining: 15.9s\n",
      "808:\ttotal: 1m 7s\tremaining: 15.8s\n",
      "809:\ttotal: 1m 7s\tremaining: 15.7s\n",
      "810:\ttotal: 1m 7s\tremaining: 15.7s\n",
      "811:\ttotal: 1m 7s\tremaining: 15.6s\n",
      "812:\ttotal: 1m 7s\tremaining: 15.5s\n",
      "813:\ttotal: 1m 7s\tremaining: 15.4s\n",
      "814:\ttotal: 1m 7s\tremaining: 15.3s\n",
      "815:\ttotal: 1m 7s\tremaining: 15.2s\n",
      "816:\ttotal: 1m 7s\tremaining: 15.1s\n",
      "817:\ttotal: 1m 7s\tremaining: 15.1s\n",
      "818:\ttotal: 1m 7s\tremaining: 15s\n",
      "819:\ttotal: 1m 7s\tremaining: 14.9s\n",
      "820:\ttotal: 1m 7s\tremaining: 14.8s\n",
      "821:\ttotal: 1m 7s\tremaining: 14.7s\n",
      "822:\ttotal: 1m 8s\tremaining: 14.6s\n",
      "823:\ttotal: 1m 8s\tremaining: 14.6s\n",
      "824:\ttotal: 1m 8s\tremaining: 14.5s\n",
      "825:\ttotal: 1m 8s\tremaining: 14.4s\n",
      "826:\ttotal: 1m 8s\tremaining: 14.3s\n",
      "827:\ttotal: 1m 8s\tremaining: 14.2s\n",
      "828:\ttotal: 1m 8s\tremaining: 14.1s\n",
      "829:\ttotal: 1m 8s\tremaining: 14.1s\n",
      "830:\ttotal: 1m 8s\tremaining: 14s\n",
      "831:\ttotal: 1m 8s\tremaining: 13.9s\n",
      "832:\ttotal: 1m 8s\tremaining: 13.8s\n",
      "833:\ttotal: 1m 8s\tremaining: 13.7s\n",
      "834:\ttotal: 1m 9s\tremaining: 13.6s\n",
      "835:\ttotal: 1m 9s\tremaining: 13.6s\n",
      "836:\ttotal: 1m 9s\tremaining: 13.5s\n",
      "837:\ttotal: 1m 9s\tremaining: 13.4s\n",
      "838:\ttotal: 1m 9s\tremaining: 13.3s\n",
      "839:\ttotal: 1m 9s\tremaining: 13.2s\n",
      "840:\ttotal: 1m 9s\tremaining: 13.1s\n",
      "841:\ttotal: 1m 9s\tremaining: 13.1s\n",
      "842:\ttotal: 1m 9s\tremaining: 13s\n",
      "843:\ttotal: 1m 9s\tremaining: 12.9s\n",
      "844:\ttotal: 1m 9s\tremaining: 12.8s\n",
      "845:\ttotal: 1m 9s\tremaining: 12.7s\n",
      "846:\ttotal: 1m 9s\tremaining: 12.6s\n",
      "847:\ttotal: 1m 10s\tremaining: 12.6s\n",
      "848:\ttotal: 1m 10s\tremaining: 12.5s\n",
      "849:\ttotal: 1m 10s\tremaining: 12.4s\n",
      "850:\ttotal: 1m 10s\tremaining: 12.3s\n",
      "851:\ttotal: 1m 10s\tremaining: 12.2s\n",
      "852:\ttotal: 1m 10s\tremaining: 12.1s\n",
      "853:\ttotal: 1m 10s\tremaining: 12.1s\n",
      "854:\ttotal: 1m 10s\tremaining: 12s\n",
      "855:\ttotal: 1m 10s\tremaining: 11.9s\n",
      "856:\ttotal: 1m 10s\tremaining: 11.8s\n",
      "857:\ttotal: 1m 10s\tremaining: 11.7s\n",
      "858:\ttotal: 1m 11s\tremaining: 11.7s\n",
      "859:\ttotal: 1m 11s\tremaining: 11.6s\n",
      "860:\ttotal: 1m 11s\tremaining: 11.5s\n",
      "861:\ttotal: 1m 11s\tremaining: 11.4s\n",
      "862:\ttotal: 1m 11s\tremaining: 11.3s\n",
      "863:\ttotal: 1m 11s\tremaining: 11.2s\n",
      "864:\ttotal: 1m 11s\tremaining: 11.2s\n",
      "865:\ttotal: 1m 11s\tremaining: 11.1s\n",
      "866:\ttotal: 1m 11s\tremaining: 11s\n",
      "867:\ttotal: 1m 11s\tremaining: 10.9s\n",
      "868:\ttotal: 1m 11s\tremaining: 10.8s\n",
      "869:\ttotal: 1m 11s\tremaining: 10.7s\n",
      "870:\ttotal: 1m 11s\tremaining: 10.7s\n",
      "871:\ttotal: 1m 11s\tremaining: 10.6s\n",
      "872:\ttotal: 1m 12s\tremaining: 10.5s\n",
      "873:\ttotal: 1m 12s\tremaining: 10.4s\n",
      "874:\ttotal: 1m 12s\tremaining: 10.3s\n",
      "875:\ttotal: 1m 12s\tremaining: 10.2s\n",
      "876:\ttotal: 1m 12s\tremaining: 10.1s\n",
      "877:\ttotal: 1m 12s\tremaining: 10.1s\n",
      "878:\ttotal: 1m 12s\tremaining: 9.98s\n",
      "879:\ttotal: 1m 12s\tremaining: 9.9s\n",
      "880:\ttotal: 1m 12s\tremaining: 9.82s\n",
      "881:\ttotal: 1m 12s\tremaining: 9.74s\n",
      "882:\ttotal: 1m 12s\tremaining: 9.66s\n",
      "883:\ttotal: 1m 12s\tremaining: 9.57s\n",
      "884:\ttotal: 1m 13s\tremaining: 9.49s\n",
      "885:\ttotal: 1m 13s\tremaining: 9.41s\n",
      "886:\ttotal: 1m 13s\tremaining: 9.33s\n",
      "887:\ttotal: 1m 13s\tremaining: 9.24s\n",
      "888:\ttotal: 1m 13s\tremaining: 9.16s\n",
      "889:\ttotal: 1m 13s\tremaining: 9.08s\n",
      "890:\ttotal: 1m 13s\tremaining: 9s\n",
      "891:\ttotal: 1m 13s\tremaining: 8.91s\n",
      "892:\ttotal: 1m 13s\tremaining: 8.83s\n",
      "893:\ttotal: 1m 13s\tremaining: 8.75s\n",
      "894:\ttotal: 1m 13s\tremaining: 8.66s\n",
      "895:\ttotal: 1m 13s\tremaining: 8.58s\n",
      "896:\ttotal: 1m 13s\tremaining: 8.5s\n",
      "897:\ttotal: 1m 14s\tremaining: 8.42s\n",
      "898:\ttotal: 1m 14s\tremaining: 8.33s\n",
      "899:\ttotal: 1m 14s\tremaining: 8.25s\n",
      "900:\ttotal: 1m 14s\tremaining: 8.17s\n",
      "901:\ttotal: 1m 14s\tremaining: 8.09s\n",
      "902:\ttotal: 1m 14s\tremaining: 8s\n",
      "903:\ttotal: 1m 14s\tremaining: 7.92s\n",
      "904:\ttotal: 1m 14s\tremaining: 7.84s\n",
      "905:\ttotal: 1m 14s\tremaining: 7.76s\n",
      "906:\ttotal: 1m 14s\tremaining: 7.67s\n",
      "907:\ttotal: 1m 14s\tremaining: 7.59s\n",
      "908:\ttotal: 1m 14s\tremaining: 7.51s\n",
      "909:\ttotal: 1m 15s\tremaining: 7.42s\n",
      "910:\ttotal: 1m 15s\tremaining: 7.34s\n",
      "911:\ttotal: 1m 15s\tremaining: 7.25s\n",
      "912:\ttotal: 1m 15s\tremaining: 7.17s\n",
      "913:\ttotal: 1m 15s\tremaining: 7.09s\n",
      "914:\ttotal: 1m 15s\tremaining: 7s\n",
      "915:\ttotal: 1m 15s\tremaining: 6.92s\n",
      "916:\ttotal: 1m 15s\tremaining: 6.84s\n",
      "917:\ttotal: 1m 15s\tremaining: 6.75s\n",
      "918:\ttotal: 1m 15s\tremaining: 6.67s\n",
      "919:\ttotal: 1m 15s\tremaining: 6.58s\n",
      "920:\ttotal: 1m 15s\tremaining: 6.5s\n",
      "921:\ttotal: 1m 15s\tremaining: 6.42s\n",
      "922:\ttotal: 1m 15s\tremaining: 6.33s\n",
      "923:\ttotal: 1m 15s\tremaining: 6.25s\n",
      "924:\ttotal: 1m 16s\tremaining: 6.17s\n",
      "925:\ttotal: 1m 16s\tremaining: 6.08s\n",
      "926:\ttotal: 1m 16s\tremaining: 6s\n",
      "927:\ttotal: 1m 16s\tremaining: 5.92s\n",
      "928:\ttotal: 1m 16s\tremaining: 5.84s\n",
      "929:\ttotal: 1m 16s\tremaining: 5.75s\n",
      "930:\ttotal: 1m 16s\tremaining: 5.67s\n",
      "931:\ttotal: 1m 16s\tremaining: 5.59s\n",
      "932:\ttotal: 1m 16s\tremaining: 5.5s\n",
      "933:\ttotal: 1m 16s\tremaining: 5.42s\n",
      "934:\ttotal: 1m 16s\tremaining: 5.34s\n",
      "935:\ttotal: 1m 16s\tremaining: 5.26s\n",
      "936:\ttotal: 1m 16s\tremaining: 5.17s\n",
      "937:\ttotal: 1m 17s\tremaining: 5.09s\n",
      "938:\ttotal: 1m 17s\tremaining: 5.01s\n",
      "939:\ttotal: 1m 17s\tremaining: 4.93s\n",
      "940:\ttotal: 1m 17s\tremaining: 4.85s\n",
      "941:\ttotal: 1m 17s\tremaining: 4.76s\n",
      "942:\ttotal: 1m 17s\tremaining: 4.68s\n",
      "943:\ttotal: 1m 17s\tremaining: 4.6s\n",
      "944:\ttotal: 1m 17s\tremaining: 4.52s\n",
      "945:\ttotal: 1m 17s\tremaining: 4.44s\n",
      "946:\ttotal: 1m 17s\tremaining: 4.36s\n",
      "947:\ttotal: 1m 17s\tremaining: 4.27s\n",
      "948:\ttotal: 1m 17s\tremaining: 4.19s\n",
      "949:\ttotal: 1m 18s\tremaining: 4.11s\n",
      "950:\ttotal: 1m 18s\tremaining: 4.03s\n",
      "951:\ttotal: 1m 18s\tremaining: 3.94s\n",
      "952:\ttotal: 1m 18s\tremaining: 3.86s\n",
      "953:\ttotal: 1m 18s\tremaining: 3.78s\n",
      "954:\ttotal: 1m 18s\tremaining: 3.7s\n",
      "955:\ttotal: 1m 18s\tremaining: 3.61s\n",
      "956:\ttotal: 1m 18s\tremaining: 3.53s\n",
      "957:\ttotal: 1m 18s\tremaining: 3.45s\n",
      "958:\ttotal: 1m 18s\tremaining: 3.37s\n",
      "959:\ttotal: 1m 18s\tremaining: 3.28s\n",
      "960:\ttotal: 1m 18s\tremaining: 3.2s\n",
      "961:\ttotal: 1m 19s\tremaining: 3.12s\n",
      "962:\ttotal: 1m 19s\tremaining: 3.04s\n",
      "963:\ttotal: 1m 19s\tremaining: 2.96s\n",
      "964:\ttotal: 1m 19s\tremaining: 2.88s\n",
      "965:\ttotal: 1m 19s\tremaining: 2.79s\n",
      "966:\ttotal: 1m 19s\tremaining: 2.71s\n",
      "967:\ttotal: 1m 19s\tremaining: 2.63s\n",
      "968:\ttotal: 1m 19s\tremaining: 2.55s\n",
      "969:\ttotal: 1m 19s\tremaining: 2.46s\n",
      "970:\ttotal: 1m 19s\tremaining: 2.38s\n",
      "971:\ttotal: 1m 19s\tremaining: 2.3s\n",
      "972:\ttotal: 1m 19s\tremaining: 2.22s\n",
      "973:\ttotal: 1m 20s\tremaining: 2.14s\n",
      "974:\ttotal: 1m 20s\tremaining: 2.06s\n",
      "975:\ttotal: 1m 20s\tremaining: 1.97s\n",
      "976:\ttotal: 1m 20s\tremaining: 1.89s\n",
      "977:\ttotal: 1m 20s\tremaining: 1.81s\n",
      "978:\ttotal: 1m 20s\tremaining: 1.73s\n",
      "979:\ttotal: 1m 20s\tremaining: 1.64s\n",
      "980:\ttotal: 1m 20s\tremaining: 1.56s\n",
      "981:\ttotal: 1m 20s\tremaining: 1.48s\n",
      "982:\ttotal: 1m 20s\tremaining: 1.4s\n",
      "983:\ttotal: 1m 20s\tremaining: 1.31s\n",
      "984:\ttotal: 1m 20s\tremaining: 1.23s\n",
      "985:\ttotal: 1m 20s\tremaining: 1.15s\n",
      "986:\ttotal: 1m 20s\tremaining: 1.07s\n",
      "987:\ttotal: 1m 21s\tremaining: 984ms\n",
      "988:\ttotal: 1m 21s\tremaining: 902ms\n",
      "989:\ttotal: 1m 21s\tremaining: 820ms\n",
      "990:\ttotal: 1m 21s\tremaining: 738ms\n",
      "991:\ttotal: 1m 21s\tremaining: 656ms\n",
      "992:\ttotal: 1m 21s\tremaining: 574ms\n",
      "993:\ttotal: 1m 21s\tremaining: 492ms\n",
      "994:\ttotal: 1m 21s\tremaining: 410ms\n",
      "995:\ttotal: 1m 21s\tremaining: 328ms\n",
      "996:\ttotal: 1m 21s\tremaining: 246ms\n",
      "997:\ttotal: 1m 21s\tremaining: 164ms\n",
      "998:\ttotal: 1m 21s\tremaining: 82ms\n",
      "999:\ttotal: 1m 21s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f2d56d0f2e0>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.4\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "db53ecc8",
   "metadata": {
    "cellId": "a0dks47olwefoy1ziaudz"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "predict = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "98afd7ff",
   "metadata": {
    "cellId": "hc7kjgyrc1rmbuyeqqqi6k"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "with open ('predict_ans.npy', 'wb') as f:\n",
    "    np.save(f, (predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c262d7fa",
   "metadata": {
    "cellId": "c18555u04m0pz1xrukdmij"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC score: 0.94\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'border_count': 32,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': 42,\n",
    "    'verbose': False\n",
    "    \n",
    "}\n",
    "model_cv = CatBoostClassifier(**params)\n",
    "cv_scores = cross_val_score(model_cv, x_train, df['sentiment'], cv=6, scoring='roc_auc_ovr')\n",
    "mean_cv_score = cv_scores.mean()\n",
    "print(f'Mean ROC-AUC score: {mean_cv_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "9d3bb504",
   "metadata": {
    "cellId": "vjq8ry3lrifc3tvj058rk5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /home/jupyter/work)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Process exited with code 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8cc023078b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git remote'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_output_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_message_handlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, lang, code)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process exited with code %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Process exited with code 128"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "!git remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd3426",
   "metadata": {
    "cellId": "uev0cu28f8sq35olfjyww"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "0903cb7b-9299-46d6-a22c-a4b74edeb8dc",
  "notebookPath": "Hackathon-Sentiment-Analysis/BERT.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
